{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d329b8",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - Training Pipeline (Colab)\n",
    "\n",
    "DINOv2 + Tabular Fusion Model for Multi-output Regression\n",
    "\n",
    "**Strategy:**\n",
    "1. DINOv2-base backbone (fine-tuned)\n",
    "2. Multi-scale feature extraction (layers 6, 9, 12)\n",
    "3. Cross-attention fusion with tabular features\n",
    "4. Multi-task regression head\n",
    "5. Log transform for targets\n",
    "6. State-based CV (spatial split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246991c5",
   "metadata": {},
   "source": [
    "## 0. Setup (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm albumentations transformers accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Colab에서 Drive 마운트 (필요시 주석 해제)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58abc0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "class CFG:\n",
    "    # Paths - Colab 환경에 맞게 수정\n",
    "    data_dir = Path(\"./data\")  # 또는 \"/content/drive/MyDrive/kaggle/csiro-biomass/data\"\n",
    "    output_dir = Path(\"./outputs\")\n",
    "\n",
    "    # Model\n",
    "    backbone = \"vit_base_patch14_dinov2.lvd142m\"  # DINOv2-base\n",
    "    img_size = 518  # DINOv2 default\n",
    "    in_chans = 3\n",
    "\n",
    "    # Tabular\n",
    "    num_tabular_features = 2  # NDVI, Height\n",
    "    num_cat_features = 2  # State, Species\n",
    "\n",
    "    # Targets\n",
    "    targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'Dry_Total_g', 'GDM_g']\n",
    "    num_targets = 5\n",
    "\n",
    "    # Training\n",
    "    n_folds = 4\n",
    "    train_folds = [0]  # 빠른 실험용. 전체는 [0,1,2,3]\n",
    "    epochs = 15\n",
    "    batch_size = 8\n",
    "    lr = 1e-4\n",
    "    backbone_lr = 1e-5  # backbone은 더 낮은 lr\n",
    "    weight_decay = 1e-2\n",
    "\n",
    "    # Augmentation\n",
    "    aug_prob = 0.5\n",
    "\n",
    "    # Log transform (right-skewed targets)\n",
    "    use_log_transform = True\n",
    "    log_eps = 1.0  # log(x + eps) for numerical stability\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Reproducibility\n",
    "    seed = 42\n",
    "\n",
    "    # Misc\n",
    "    num_workers = 2\n",
    "    gradient_accumulation = 2\n",
    "    mixed_precision = True\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "CFG.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Device: {CFG.device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c1e96",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99376df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(cfg: CFG):\n",
    "    \"\"\"Load and prepare data in wide format.\"\"\"\n",
    "    train_df = pd.read_csv(cfg.data_dir / \"train.csv\")\n",
    "\n",
    "    # Long → Wide format\n",
    "    tabular_cols = ['image_path', 'Sampling_Date', 'State', 'Species',\n",
    "                    'Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "\n",
    "    train_wide = train_df.pivot_table(\n",
    "        index=tabular_cols,\n",
    "        columns='target_name',\n",
    "        values='target',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Extract image_id for grouping\n",
    "    train_wide['image_id'] = train_wide['image_path'].apply(\n",
    "        lambda x: Path(x).stem\n",
    "    )\n",
    "\n",
    "    # Parse date features\n",
    "    train_wide['Sampling_Date'] = pd.to_datetime(train_wide['Sampling_Date'])\n",
    "    train_wide['Month'] = train_wide['Sampling_Date'].dt.month\n",
    "\n",
    "    # Encode categoricals\n",
    "    le_state = LabelEncoder()\n",
    "    le_species = LabelEncoder()\n",
    "    train_wide['State_enc'] = le_state.fit_transform(train_wide['State'])\n",
    "    train_wide['Species_enc'] = le_species.fit_transform(train_wide['Species'])\n",
    "\n",
    "    # Create folds (GroupKFold by State for spatial CV)\n",
    "    # Insight: 같은 State 데이터가 train/val에 섞이면 data leakage 발생\n",
    "    gkf = GroupKFold(n_splits=cfg.n_folds)\n",
    "    train_wide['fold'] = -1\n",
    "    for fold, (_, val_idx) in enumerate(gkf.split(train_wide, groups=train_wide['State'])):\n",
    "        train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "    print(f\"Data shape: {train_wide.shape}\")\n",
    "    print(f\"Fold distribution:\\n{train_wide['fold'].value_counts().sort_index()}\")\n",
    "    print(f\"States: {le_state.classes_}\")\n",
    "    print(f\"Species: {le_species.classes_}\")\n",
    "\n",
    "    return train_wide, le_state, le_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23d8d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_df, le_state, le_species = prepare_data(CFG)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1085894",
   "metadata": {},
   "source": [
    "## 2. Dataset & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2874708",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_transforms(cfg: CFG, mode='train'):\n",
    "    \"\"\"Get augmentation transforms.\"\"\"\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(cfg.img_size, cfg.img_size),\n",
    "            A.HorizontalFlip(p=cfg.aug_prob),\n",
    "            A.VerticalFlip(p=cfg.aug_prob),\n",
    "            A.RandomRotate90(p=cfg.aug_prob),\n",
    "            # Insight: 색상 변환은 약하게 (녹색/갈색 구분이 중요)\n",
    "            A.ColorJitter(\n",
    "                brightness=0.1, contrast=0.1,\n",
    "                saturation=0.1, hue=0.02,\n",
    "                p=cfg.aug_prob * 0.5\n",
    "            ),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1, scale_limit=0.1, rotate_limit=15,\n",
    "                border_mode=0, p=cfg.aug_prob\n",
    "            ),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(cfg.img_size, cfg.img_size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d863dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, cfg: CFG, transforms=None, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = self.cfg.data_dir / row['image_path']\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "\n",
    "        # Tabular features (numerical)\n",
    "        tabular = torch.tensor([\n",
    "            row['Pre_GSHH_NDVI'],\n",
    "            row['Height_Ave_cm'] / 100.0,  # Normalize height\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # Categorical features\n",
    "        cat_features = torch.tensor([\n",
    "            row['State_enc'],\n",
    "            row['Species_enc'],\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.tensor([\n",
    "            row[t] for t in self.cfg.targets\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # Log transform targets\n",
    "        if self.cfg.use_log_transform:\n",
    "            targets = torch.log1p(targets)  # log(1 + x)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'tabular': tabular,\n",
    "            'cat_features': cat_features,\n",
    "            'targets': targets,\n",
    "            'image_id': row['image_id']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777cf45",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7c2de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiScaleDINOv2(nn.Module):\n",
    "    \"\"\"\n",
    "    DINOv2 with multi-scale feature extraction.\n",
    "    Extracts features from multiple transformer layers for richer representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classification head\n",
    "        )\n",
    "        self.embed_dim = self.backbone.embed_dim  # 768 for base\n",
    "\n",
    "        # Insight: 다른 layer는 다른 수준의 feature를 캡처\n",
    "        # Early layers: 텍스처, 엣지 / Late layers: 의미론적 정보\n",
    "        self.feature_layers = [6, 9, 11]  # 0-indexed\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get intermediate features\n",
    "        features = []\n",
    "\n",
    "        # Patch embedding\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        x = self.backbone._pos_embed(x)\n",
    "        x = self.backbone.patch_drop(x)\n",
    "        x = self.backbone.norm_pre(x)\n",
    "\n",
    "        for i, block in enumerate(self.backbone.blocks):\n",
    "            x = block(x)\n",
    "            if i in self.feature_layers:\n",
    "                # CLS token\n",
    "                features.append(x[:, 0])\n",
    "\n",
    "        # Final layer\n",
    "        x = self.backbone.norm(x)\n",
    "        features.append(x[:, 0])\n",
    "\n",
    "        # Concat multi-scale features\n",
    "        # Shape: (B, embed_dim * num_layers)\n",
    "        multi_scale_feat = torch.cat(features, dim=-1)\n",
    "\n",
    "        return multi_scale_feat, x[:, 1:]  # CLS features, patch tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042b6e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TabularEncoder(nn.Module):\n",
    "    \"\"\"Encode tabular features (numerical + categorical).\"\"\"\n",
    "    def __init__(self, num_numerical, cat_dims, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_numerical = num_numerical\n",
    "\n",
    "        # Numerical features MLP\n",
    "        self.num_encoder = nn.Sequential(\n",
    "            nn.Linear(num_numerical, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        # Categorical embeddings\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_classes, embed_dim)\n",
    "            for num_classes in cat_dims\n",
    "        ])\n",
    "\n",
    "        # Combine\n",
    "        total_dim = embed_dim * (1 + len(cat_dims))\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(total_dim, embed_dim * 2),\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, numerical, categorical):\n",
    "        # Numerical\n",
    "        num_feat = self.num_encoder(numerical)\n",
    "\n",
    "        # Categorical\n",
    "        cat_feats = [emb(categorical[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([num_feat] + cat_feats, dim=-1)\n",
    "        return self.combine(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf60e2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention to fuse tabular features with image patch tokens.\n",
    "    Tabular queries attend to image patches to find relevant regions.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim, tab_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = img_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Project tabular to query\n",
    "        self.q_proj = nn.Linear(tab_dim, img_dim)\n",
    "        # Image patches as key/value\n",
    "        self.k_proj = nn.Linear(img_dim, img_dim)\n",
    "        self.v_proj = nn.Linear(img_dim, img_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(img_dim, img_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tab_feat, img_tokens):\n",
    "        \"\"\"\n",
    "        tab_feat: (B, tab_dim) - tabular features\n",
    "        img_tokens: (B, N, img_dim) - image patch tokens\n",
    "        \"\"\"\n",
    "        B, N, C = img_tokens.shape\n",
    "\n",
    "        # Expand tabular to sequence (B, 1, img_dim)\n",
    "        q = self.q_proj(tab_feat).unsqueeze(1)\n",
    "        k = self.k_proj(img_tokens)\n",
    "        v = self.v_proj(img_tokens)\n",
    "\n",
    "        # Multi-head attention\n",
    "        q = q.view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Insight: F.scaled_dot_product_attention uses FlashAttention when available\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout.p if self.training else 0.0)\n",
    "\n",
    "        attn_out = attn_out.transpose(1, 2).reshape(B, 1, C)\n",
    "        attn_out = self.out_proj(attn_out).squeeze(1)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e63ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model: DINOv2 + Tabular + Cross-Attention Fusion\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: CFG, cat_dims):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Image encoder (DINOv2)\n",
    "        self.image_encoder = MultiScaleDINOv2(cfg.backbone, pretrained=True)\n",
    "        img_dim = self.image_encoder.embed_dim\n",
    "        multi_scale_dim = img_dim * 4  # 4 layers of features\n",
    "\n",
    "        # Tabular encoder\n",
    "        tab_embed_dim = 128\n",
    "        self.tabular_encoder = TabularEncoder(\n",
    "            num_numerical=cfg.num_tabular_features,\n",
    "            cat_dims=cat_dims,\n",
    "            embed_dim=tab_embed_dim\n",
    "        )\n",
    "\n",
    "        # Cross-attention fusion\n",
    "        self.cross_attn = CrossAttentionFusion(\n",
    "            img_dim=img_dim,\n",
    "            tab_dim=tab_embed_dim,\n",
    "            num_heads=8\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        fusion_dim = multi_scale_dim + tab_embed_dim + img_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Multi-task regression head\n",
    "        self.head = nn.Linear(256, cfg.num_targets)\n",
    "\n",
    "    def forward(self, image, tabular, cat_features):\n",
    "        # Image features\n",
    "        multi_scale_feat, patch_tokens = self.image_encoder(image)\n",
    "\n",
    "        # Tabular features\n",
    "        tab_feat = self.tabular_encoder(tabular, cat_features)\n",
    "\n",
    "        # Cross-attention: tabular queries image patches\n",
    "        cross_attn_feat = self.cross_attn(tab_feat, patch_tokens)\n",
    "\n",
    "        # Fuse all features\n",
    "        fused = torch.cat([multi_scale_feat, tab_feat, cross_attn_feat], dim=-1)\n",
    "        fused = self.fusion(fused)\n",
    "\n",
    "        # Predict all targets\n",
    "        out = self.head(fused)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d7bbc",
   "metadata": {},
   "source": [
    "## 4. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c3c22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for multi-output regression.\n",
    "    - SmoothL1 (Huber) for robustness to outliers\n",
    "    - Physics constraint: Total ≈ Green + Dead + Clover\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: CFG, physics_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.physics_weight = physics_weight\n",
    "        self.smooth_l1 = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "        # Target weights (Clover가 어려우므로 가중치 높임)\n",
    "        # [Green, Dead, Clover, Total, GDM]\n",
    "        self.target_weights = torch.tensor([1.0, 1.0, 1.5, 1.0, 1.0])\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: (B, 5) - predicted values (log-transformed)\n",
    "        target: (B, 5) - target values (log-transformed)\n",
    "        \"\"\"\n",
    "        # Per-target loss\n",
    "        loss_per_target = self.smooth_l1(pred, target)  # (B, 5)\n",
    "\n",
    "        # Weighted mean\n",
    "        weights = self.target_weights.to(pred.device)\n",
    "        weighted_loss = (loss_per_target * weights).mean()\n",
    "\n",
    "        # Physics constraint (in original scale)\n",
    "        if self.physics_weight > 0:\n",
    "            # Convert back from log scale\n",
    "            pred_orig = torch.expm1(pred)  # exp(x) - 1\n",
    "\n",
    "            # Total should equal Green + Dead + Clover\n",
    "            pred_sum = pred_orig[:, 0] + pred_orig[:, 1] + pred_orig[:, 2]\n",
    "            pred_total = pred_orig[:, 3]\n",
    "            physics_loss = F.smooth_l1_loss(pred_sum, pred_total)\n",
    "\n",
    "            weighted_loss = weighted_loss + self.physics_weight * physics_loss\n",
    "\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78637a2",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c06190",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, scheduler, criterion, cfg, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "        image = batch['image'].to(cfg.device)\n",
    "        tabular = batch['tabular'].to(cfg.device)\n",
    "        cat_features = batch['cat_features'].to(cfg.device)\n",
    "        targets = batch['targets'].to(cfg.device)\n",
    "\n",
    "        # Mixed precision training\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.mixed_precision):\n",
    "            pred = model(image, tabular, cat_features)\n",
    "            loss = criterion(pred, targets)\n",
    "            loss = loss / cfg.gradient_accumulation\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % cfg.gradient_accumulation == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * cfg.gradient_accumulation\n",
    "        pbar.set_postfix({'loss': f'{loss.item() * cfg.gradient_accumulation:.4f}'})\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2f1b5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, cfg):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(loader, desc='Validation'):\n",
    "        image = batch['image'].to(cfg.device)\n",
    "        tabular = batch['tabular'].to(cfg.device)\n",
    "        cat_features = batch['cat_features'].to(cfg.device)\n",
    "        targets = batch['targets'].to(cfg.device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.mixed_precision):\n",
    "            pred = model(image, tabular, cat_features)\n",
    "            loss = criterion(pred, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert back from log scale\n",
    "        if cfg.use_log_transform:\n",
    "            pred = torch.expm1(pred)\n",
    "            targets_orig = torch.expm1(targets)\n",
    "        else:\n",
    "            targets_orig = targets\n",
    "\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_targets.append(targets_orig.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
    "\n",
    "    # Compute R² for each target\n",
    "    r2_scores = {}\n",
    "    for i, target_name in enumerate(cfg.targets):\n",
    "        ss_res = np.sum((all_targets[:, i] - all_preds[:, i]) ** 2)\n",
    "        ss_tot = np.sum((all_targets[:, i] - all_targets[:, i].mean()) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        r2_scores[target_name] = r2\n",
    "\n",
    "    # Global weighted R² (equal weights for now)\n",
    "    mean_r2 = np.mean(list(r2_scores.values()))\n",
    "\n",
    "    return total_loss / len(loader), r2_scores, mean_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d51337",
   "metadata": {},
   "source": [
    "## 6. Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e0284",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_fold(fold, train_df, cfg):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Split data\n",
    "    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "    print(f\"Val States: {val_data['State'].unique()}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = BiomassDataset(\n",
    "        train_data, cfg,\n",
    "        transforms=get_transforms(cfg, 'train'),\n",
    "        mode='train'\n",
    "    )\n",
    "    val_dataset = BiomassDataset(\n",
    "        val_data, cfg,\n",
    "        transforms=get_transforms(cfg, 'val'),\n",
    "        mode='val'\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    cat_dims = [\n",
    "        train_df['State_enc'].nunique(),\n",
    "        train_df['Species_enc'].nunique()\n",
    "    ]\n",
    "    model = BiomassModel(cfg, cat_dims).to(cfg.device)\n",
    "\n",
    "    # Optimizer with different LR for backbone\n",
    "    backbone_params = list(model.image_encoder.parameters())\n",
    "    other_params = [p for n, p in model.named_parameters() if 'image_encoder' not in n]\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {'params': backbone_params, 'lr': cfg.backbone_lr},\n",
    "        {'params': other_params, 'lr': cfg.lr}\n",
    "    ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # Scheduler\n",
    "    num_training_steps = len(train_loader) * cfg.epochs // cfg.gradient_accumulation\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_training_steps, eta_min=1e-7)\n",
    "\n",
    "    # Loss\n",
    "    criterion = BiomassLoss(cfg, physics_weight=0.1)\n",
    "\n",
    "    # Mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler() if cfg.mixed_precision else None\n",
    "\n",
    "    # Training loop\n",
    "    best_r2 = -float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_r2': []}\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, cfg, scaler\n",
    "        )\n",
    "        val_loss, r2_scores, mean_r2 = validate(model, val_loader, criterion, cfg)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_r2'].append(mean_r2)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val R² (mean): {mean_r2:.4f}\")\n",
    "        for name, r2 in r2_scores.items():\n",
    "            print(f\"  {name}: {r2:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if mean_r2 > best_r2:\n",
    "            best_r2 = mean_r2\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_r2': best_r2,\n",
    "                'r2_scores': r2_scores,\n",
    "                'cat_dims': cat_dims,\n",
    "            }, cfg.output_dir / f'best_model_fold{fold}.pt')\n",
    "            print(f\"✓ Saved best model (R²: {best_r2:.4f})\")\n",
    "\n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title('Loss Curve')\n",
    "\n",
    "    axes[1].plot(history['val_r2'], label='Val R²', color='green')\n",
    "    axes[1].axhline(y=best_r2, color='r', linestyle='--', label=f'Best: {best_r2:.4f}')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('R²')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title('Validation R²')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cfg.output_dir / f'training_history_fold{fold}.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    return best_r2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6418e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train all folds\n",
    "all_scores = []\n",
    "for fold in CFG.train_folds:\n",
    "    best_r2, _ = train_fold(fold, train_df, CFG)\n",
    "    all_scores.append(best_r2)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Mean CV R²: {np.mean(all_scores):.4f} ± {np.std(all_scores):.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ace812",
   "metadata": {},
   "source": [
    "## 7. Inference (Kaggle Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c55ee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_tta(model, image, tabular, cat_features, cfg, n_tta=8):\n",
    "    \"\"\"Test-Time Augmentation for robust predictions.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    # Original\n",
    "    pred = model(image, tabular, cat_features)\n",
    "    predictions.append(pred)\n",
    "\n",
    "    # TTA augmentations\n",
    "    if n_tta > 1:\n",
    "        # Horizontal flip\n",
    "        pred = model(torch.flip(image, dims=[3]), tabular, cat_features)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    if n_tta > 2:\n",
    "        # Vertical flip\n",
    "        pred = model(torch.flip(image, dims=[2]), tabular, cat_features)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    if n_tta > 4:\n",
    "        # HFlip + VFlip\n",
    "        pred = model(torch.flip(image, dims=[2, 3]), tabular, cat_features)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Average predictions\n",
    "    pred_mean = torch.stack(predictions).mean(dim=0)\n",
    "\n",
    "    # Convert from log scale\n",
    "    if cfg.use_log_transform:\n",
    "        pred_mean = torch.expm1(pred_mean)\n",
    "\n",
    "    return pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3fe4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_submission(model, test_df, cfg, le_state, le_species):\n",
    "    \"\"\"Create submission.csv from model predictions.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare test data (wide format)\n",
    "    test_wide = test_df.copy()\n",
    "    test_wide = test_wide.drop_duplicates(subset=['image_path'])\n",
    "\n",
    "    # [IMPORTANT] test.csv에는 tabular features가 없음!\n",
    "    # Train mean 값으로 대체 (또는 Image-only 모델 사용 권장)\n",
    "    has_tabular = 'Pre_GSHH_NDVI' in test_wide.columns\n",
    "\n",
    "    if not has_tabular:\n",
    "        print(\"⚠️  Warning: No tabular features in test.csv!\")\n",
    "        print(\"   Using train mean values as placeholder.\")\n",
    "        test_wide['Pre_GSHH_NDVI'] = 0.657  # train mean\n",
    "        test_wide['Height_Ave_cm'] = 7.596  # train mean\n",
    "        test_wide['State'] = 'Vic'  # most common\n",
    "        test_wide['Species'] = 'Ryegrass_Clover'  # most common\n",
    "\n",
    "    # Encode (handle unseen categories)\n",
    "    test_wide['State_enc'] = test_wide['State'].apply(\n",
    "        lambda x: le_state.transform([x])[0] if x in le_state.classes_ else 0\n",
    "    )\n",
    "    test_wide['Species_enc'] = test_wide['Species'].apply(\n",
    "        lambda x: le_species.transform([x])[0] if x in le_species.classes_ else 0\n",
    "    )\n",
    "\n",
    "    transforms = get_transforms(cfg, 'val')\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in tqdm(test_wide.iterrows(), total=len(test_wide), desc='Inference'):\n",
    "        # Load image\n",
    "        img_path = cfg.data_dir / row['image_path']\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        image = transforms(image=image)['image']\n",
    "        image = image.unsqueeze(0).to(cfg.device)\n",
    "\n",
    "        # Tabular\n",
    "        tabular = torch.tensor([[\n",
    "            row['Pre_GSHH_NDVI'],\n",
    "            row['Height_Ave_cm'] / 100.0\n",
    "        ]], dtype=torch.float32).to(cfg.device)\n",
    "\n",
    "        cat_features = torch.tensor([[\n",
    "            row['State_enc'],\n",
    "            row['Species_enc']\n",
    "        ]], dtype=torch.long).to(cfg.device)\n",
    "\n",
    "        # Predict with TTA\n",
    "        pred = predict_tta(model, image, tabular, cat_features, cfg, n_tta=4)\n",
    "        predictions.append(pred.cpu().numpy()[0])\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Create submission in long format\n",
    "    submission_rows = []\n",
    "    for i, (_, row) in enumerate(test_wide.iterrows()):\n",
    "        img_id = Path(row['image_path']).stem\n",
    "        for j, target_name in enumerate(cfg.targets):\n",
    "            submission_rows.append({\n",
    "                'sample_id': f\"{img_id}__{target_name}\",\n",
    "                'target': max(0, predictions[i, j])  # Ensure non-negative\n",
    "            })\n",
    "\n",
    "    submission = pd.DataFrame(submission_rows)\n",
    "    submission.to_csv(cfg.output_dir / 'submission.csv', index=False)\n",
    "    print(f\"Submission saved: {len(submission)} rows\")\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and create submission\n",
    "checkpoint = torch.load(CFG.output_dir / f'best_model_fold{CFG.train_folds[0]}.pt')\n",
    "cat_dims = checkpoint['cat_dims']\n",
    "\n",
    "model = BiomassModel(CFG, cat_dims).to(CFG.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model with R²: {checkpoint['best_r2']:.4f}\")\n",
    "\n",
    "# Create submission (using test.csv)\n",
    "test_df = pd.read_csv(CFG.data_dir / 'test.csv')\n",
    "submission = create_submission(model, test_df, CFG, le_state, le_species)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322fecd",
   "metadata": {},
   "source": [
    "## 8. Save for Kaggle Upload\n",
    "\n",
    "1. `outputs/best_model_fold0.pt` → Kaggle Dataset 업로드\n",
    "2. 아래 inference 코드를 Kaggle Notebook에 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "============================================================\n",
    "Kaggle 제출 방법\n",
    "============================================================\n",
    "\n",
    "1. 모델 weights 업로드:\n",
    "   - outputs/best_model_fold0.pt → Kaggle Datasets에 업로드\n",
    "   - 이름: \"csiro-biomass-weights\" 등\n",
    "\n",
    "2. Kaggle Notebook 생성:\n",
    "   - 아래 inference 코드 복사\n",
    "   - Add Data: 업로드한 weights dataset 추가\n",
    "   - GPU 활성화\n",
    "   - Internet Off\n",
    "   - Submit\n",
    "\n",
    "3. Inference 코드 (Kaggle용):\n",
    "============================================================\n",
    "\"\"\")\n",
    "\n",
    "kaggle_inference_code = '''\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# [Model definition code here - copy from above]\n",
    "# MultiScaleDINOv2, TabularEncoder, CrossAttentionFusion, BiomassModel classes\n",
    "\n",
    "# Config\n",
    "class CFG:\n",
    "    data_dir = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    weights_dir = Path(\"/kaggle/input/csiro-biomass-weights\")  # 업로드한 weights\n",
    "    img_size = 518\n",
    "    targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'Dry_Total_g', 'GDM_g']\n",
    "    num_targets = 5\n",
    "    use_log_transform = True\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(CFG.weights_dir / \"best_model_fold0.pt\")\n",
    "cat_dims = checkpoint['cat_dims']\n",
    "model = BiomassModel(CFG, cat_dims).to(CFG.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "test_df = pd.read_csv(CFG.data_dir / \"test.csv\")\n",
    "# ... (inference code)\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "'''\n",
    "\n",
    "print(kaggle_inference_code)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
