{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5af7624",
   "metadata": {},
   "source": [
    "# üèÜ Pseudo-Labeling Full Pipeline (5-Fold)\n",
    "\n",
    "**Ï†ÑÎûµ**: \n",
    "1. Phase 1: Image ‚Üí NDVI/Height ÏòàÏ∏° Î™®Îç∏ ÌïôÏäµ\n",
    "2. Phase 1.5: Test Ïù¥ÎØ∏ÏßÄÏóê pseudo tabular ÏÉùÏÑ±\n",
    "3. Phase 2: Image + Tabular(pseudo) ‚Üí Biomass ÏòàÏ∏° (5-Fold)\n",
    "4. Phase 3: Ensemble Ï∂îÎ°† Î∞è Ï†úÏ∂ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9db6b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e7fed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b20581",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # === Paths ===\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "    WEIGHTS_PATH = Path(\"/kaggle/input/pretrained-weights-biomass\")\n",
    "    \n",
    "    # === Model ===\n",
    "    backbone = \"efficientnet_b4\"\n",
    "    input_size = 384\n",
    "    \n",
    "    # === Phase 1: Tabular Predictor ===\n",
    "    tabular_epochs = 15\n",
    "    tabular_lr = 2e-4\n",
    "    \n",
    "    # === Phase 2: Biomass Model ===\n",
    "    n_folds = 5\n",
    "    epochs = 15\n",
    "    batch_size = 16  # GPU 2Í∞ú Í∏∞Ï§Ä\n",
    "    lr = 2e-4\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # === Features ===\n",
    "    tabular_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "    independent_targets = ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']\n",
    "    \n",
    "    # === Misc ===\n",
    "    seed = 42\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = CFG()\n",
    "cfg.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Folds: {cfg.n_folds}, Epochs: {cfg.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d0f31",
   "metadata": {},
   "source": [
    "## Competition Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecc294",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2, 'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "def competition_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    weights = np.array([TARGET_WEIGHTS[t] for t in TARGET_ORDER])\n",
    "    y_weighted_mean = sum(y_true[:, i].mean() * weights[i] for i in range(5))\n",
    "    ss_res = sum(((y_true[:, i] - y_pred[:, i]) ** 2).mean() * weights[i] for i in range(5))\n",
    "    ss_tot = sum(((y_true[:, i] - y_weighted_mean) ** 2).mean() * weights[i] for i in range(5))\n",
    "    return 1 - ss_res / (ss_tot + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c9b40",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0f1d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "    if 'target' in df.columns:\n",
    "        df_wide = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df['target'] = 0\n",
    "        cols = ['image_path']\n",
    "        for col in ['Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']:\n",
    "            if col in df.columns:\n",
    "                cols.append(col)\n",
    "        df_wide = df.drop_duplicates(subset=['image_path'])[cols].reset_index(drop=True)\n",
    "    return df_wide\n",
    "\n",
    "def get_transforms(mode: str = 'train', size: int = 384) -> A.Compose:\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.3, hue=0.05, p=0.7),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797cbdd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(cfg.DATA_PATH / \"train.csv\")\n",
    "train_wide = prepare_data(train_df, is_train=True)\n",
    "train_wide['image_id'] = train_wide['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# KFold\n",
    "kf = KFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "train_wide['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(kf.split(train_wide)):\n",
    "    train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Train data: {len(train_wide)} images\")\n",
    "print(f\"Columns: {train_wide.columns.tolist()}\")\n",
    "\n",
    "# Tabular stats\n",
    "for col in cfg.tabular_cols:\n",
    "    print(f\"  {col}: [{train_wide[col].min():.2f}, {train_wide[col].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542422a",
   "metadata": {},
   "source": [
    "## ========== PHASE 1: Tabular Predictor =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1b808",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TabularPredictorDataset(Dataset):\n",
    "    \"\"\"Image ‚Üí NDVI/Height\"\"\"\n",
    "    def __init__(self, df, cfg, transforms=None, scaler=None, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        tabular_data = df[cfg.tabular_cols].values.astype(np.float32)\n",
    "        if scaler is not None:\n",
    "            if mode == 'train':\n",
    "                self.targets = scaler.fit_transform(tabular_data)\n",
    "            else:\n",
    "                self.targets = scaler.transform(tabular_data)\n",
    "        else:\n",
    "            self.targets = tabular_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        targets = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return img, targets\n",
    "\n",
    "class SimpleImageDataset(Dataset):\n",
    "    \"\"\"Image only (for pseudo label generation)\"\"\"\n",
    "    def __init__(self, df, cfg, transforms):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32af09",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TabularPredictor(nn.Module):\n",
    "    \"\"\"Image ‚Üí NDVI/Height\"\"\"\n",
    "    def __init__(self, backbone_name, n_outputs=2, pretrained=True, weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained and weights_path and Path(weights_path).exists():\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "            weights = torch.load(weights_path, weights_only=True)\n",
    "            weights = {k: v for k, v in weights.items() if not k.startswith('classifier')}\n",
    "            self.backbone.load_state_dict(weights, strict=False)\n",
    "            print(f\"‚úì Loaded pretrained weights\")\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fa47c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_tabular_predictor(train_df, cfg):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä PHASE 1: Training Tabular Predictor (Image ‚Üí NDVI/Height)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Use fold 0 for validation\n",
    "    train_data = train_df[train_df['fold'] != 0].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_dataset = TabularPredictorDataset(\n",
    "        train_data, cfg, get_transforms('train', cfg.input_size), scaler, 'train'\n",
    "    )\n",
    "    val_dataset = TabularPredictorDataset(\n",
    "        val_data, cfg, get_transforms('val', cfg.input_size), scaler, 'val'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=cfg.num_workers)\n",
    "    \n",
    "    weights_path = None\n",
    "    if cfg.WEIGHTS_PATH.exists():\n",
    "        weights_path = str(cfg.WEIGHTS_PATH / cfg.backbone / f\"{cfg.backbone}.pth\")\n",
    "    \n",
    "    model = TabularPredictor(cfg.backbone, n_outputs=2, pretrained=True, weights_path=weights_path)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üöÄ Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(cfg.device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.tabular_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.tabular_epochs)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.tabular_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, targets in tqdm(train_loader, desc='Train', leave=False):\n",
    "            imgs, targets = imgs.to(cfg.device), targets.to(cfg.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = F.mse_loss(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs, targets = imgs.to(cfg.device), targets.to(cfg.device)\n",
    "                preds = model(imgs)\n",
    "                loss = F.mse_loss(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{cfg.tabular_epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'scaler': scaler,\n",
    "            }, cfg.OUTPUT_DIR / 'tabular_predictor.pt')\n",
    "            print(f\"  ‚úì Saved!\")\n",
    "    \n",
    "    flush()\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086d65f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tabular_scaler = train_tabular_predictor(train_wide, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ac120",
   "metadata": {},
   "source": [
    "## ========== PHASE 1.5: Generate Pseudo Labels =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ac524",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_pseudo_labels(model, loader, device, scaler):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for imgs in tqdm(loader, desc='Generating Pseudo Labels'):\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_preds = scaler.inverse_transform(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a8487",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_wide = prepare_data(test_df, is_train=False)\n",
    "print(f\"\\nTest data: {len(test_wide)} images\")\n",
    "\n",
    "# Create test loader\n",
    "test_dataset = SimpleImageDataset(test_wide, cfg, get_transforms('val', cfg.input_size))\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "# Load tabular predictor\n",
    "ckpt = torch.load(cfg.OUTPUT_DIR / 'tabular_predictor.pt', weights_only=False)\n",
    "tabular_model = TabularPredictor(cfg.backbone, n_outputs=2, pretrained=False).to(cfg.device)\n",
    "tabular_model.load_state_dict(ckpt['model_state_dict'])\n",
    "scaler = ckpt['scaler']\n",
    "\n",
    "# Generate pseudo labels\n",
    "pseudo_tabular = generate_pseudo_labels(tabular_model, test_loader, cfg.device, scaler)\n",
    "print(f\"Pseudo labels shape: {pseudo_tabular.shape}\")\n",
    "print(f\"  NDVI: [{pseudo_tabular[:, 0].min():.2f}, {pseudo_tabular[:, 0].max():.2f}]\")\n",
    "print(f\"  Height: [{pseudo_tabular[:, 1].min():.2f}, {pseudo_tabular[:, 1].max():.2f}]\")\n",
    "\n",
    "# Add to test_wide\n",
    "test_wide['Pre_GSHH_NDVI'] = pseudo_tabular[:, 0]\n",
    "test_wide['Height_Ave_cm'] = pseudo_tabular[:, 1]\n",
    "\n",
    "# Cleanup\n",
    "del tabular_model\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77ec4c",
   "metadata": {},
   "source": [
    "## ========== PHASE 2: Biomass Model with Tabular (5-Fold) =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595ad10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDatasetWithTabular(Dataset):\n",
    "    \"\"\"Image + Tabular ‚Üí Biomass\"\"\"\n",
    "    def __init__(self, df, cfg, transforms=None, mode='train', tabular_scaler=None):\n",
    "        df = df.copy()\n",
    "        for t in TARGET_ORDER:\n",
    "            if t not in df.columns:\n",
    "                df[t] = 0.0\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Tabular features\n",
    "        if all(col in df.columns for col in cfg.tabular_cols):\n",
    "            tabular_data = df[cfg.tabular_cols].values.astype(np.float32)\n",
    "            if tabular_scaler is not None:\n",
    "                if mode == 'train':\n",
    "                    self.tabular = tabular_scaler.fit_transform(tabular_data)\n",
    "                else:\n",
    "                    self.tabular = tabular_scaler.transform(tabular_data)\n",
    "            else:\n",
    "                self.tabular = tabular_data\n",
    "            self.has_tabular = True\n",
    "        else:\n",
    "            self.tabular = None\n",
    "            self.has_tabular = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        targets = torch.tensor([\n",
    "            row['Dry_Green_g'], row['Dry_Clover_g'], row['Dry_Dead_g']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        if self.has_tabular:\n",
    "            tabular = torch.tensor(self.tabular[idx], dtype=torch.float32)\n",
    "            return img, tabular, targets\n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3eba2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhysicsConstrainedHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 3)\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raw = self.head(x)\n",
    "        independent = self.softplus(raw)\n",
    "        green, clover, dead = independent[:, 0:1], independent[:, 1:2], independent[:, 2:3]\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        full = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "        return independent, full\n",
    "\n",
    "class BiomassModelWithTabular(nn.Module):\n",
    "    \"\"\"FiLM Conditioning: Image + Tabular ‚Üí Biomass\"\"\"\n",
    "    def __init__(self, backbone_name, n_tabular=2, pretrained=True, weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained and weights_path and Path(weights_path).exists():\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "            weights = torch.load(weights_path, weights_only=True)\n",
    "            weights = {k: v for k, v in weights.items() if not k.startswith('classifier')}\n",
    "            self.backbone.load_state_dict(weights, strict=False)\n",
    "            print(f\"‚úì Loaded pretrained weights\")\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        \n",
    "        # FiLM layers\n",
    "        self.tabular_encoder = nn.Sequential(\n",
    "            nn.Linear(n_tabular, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.film_gamma = nn.Linear(128, feat_dim)\n",
    "        self.film_beta = nn.Linear(128, feat_dim)\n",
    "        \n",
    "        self.head = PhysicsConstrainedHead(feat_dim)\n",
    "    \n",
    "    def forward(self, image, tabular=None):\n",
    "        img_feat = self.backbone(image)\n",
    "        \n",
    "        if tabular is not None:\n",
    "            tab_feat = self.tabular_encoder(tabular)\n",
    "            gamma = self.film_gamma(tab_feat)\n",
    "            beta = self.film_beta(tab_feat)\n",
    "            img_feat = img_feat * (1 + gamma) + beta\n",
    "        \n",
    "        return self.head(img_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561606e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_biomass_fold(fold: int, train_df: pd.DataFrame, cfg) -> Tuple[float, StandardScaler]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üåø PHASE 2: Biomass Model - Fold {fold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "    \n",
    "    tabular_scaler = StandardScaler()\n",
    "    \n",
    "    train_dataset = BiomassDatasetWithTabular(\n",
    "        train_data, cfg, get_transforms('train', cfg.input_size), 'train', tabular_scaler\n",
    "    )\n",
    "    val_dataset = BiomassDatasetWithTabular(\n",
    "        val_data, cfg, get_transforms('val', cfg.input_size), 'val', tabular_scaler\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=cfg.num_workers)\n",
    "    \n",
    "    weights_path = None\n",
    "    if cfg.WEIGHTS_PATH.exists():\n",
    "        weights_path = str(cfg.WEIGHTS_PATH / cfg.backbone / f\"{cfg.backbone}.pth\")\n",
    "    \n",
    "    model = BiomassModelWithTabular(cfg.backbone, n_tabular=2, pretrained=True, weights_path=weights_path)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üöÄ Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(cfg.device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, tabular, targets in tqdm(train_loader, desc='Train', leave=False):\n",
    "            imgs = imgs.to(cfg.device)\n",
    "            tabular = tabular.to(cfg.device)\n",
    "            targets = targets.to(cfg.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            independent, _ = model(imgs, tabular)\n",
    "            loss = F.mse_loss(independent, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, tabular, targets in val_loader:\n",
    "                imgs = imgs.to(cfg.device)\n",
    "                tabular = tabular.to(cfg.device)\n",
    "                _, full_pred = model(imgs, tabular)\n",
    "                all_preds.append(full_pred.cpu().numpy())\n",
    "                \n",
    "                green, clover, dead = targets[:, 0:1], targets[:, 1:2], targets[:, 2:3]\n",
    "                gdm = green + clover\n",
    "                total = gdm + dead\n",
    "                full_targets = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "                all_targets.append(full_targets.numpy())\n",
    "        \n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        cv_score = competition_metric(all_targets, all_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | LR: {lr:.6f} | Loss: {train_loss:.2f} | CV: {cv_score:.4f}\")\n",
    "        \n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'fold': fold,\n",
    "                'score': best_score,\n",
    "                'tabular_scaler': tabular_scaler,\n",
    "            }, cfg.OUTPUT_DIR / f'biomass_fold{fold}.pt')\n",
    "            print(f\"  ‚úì New best!\")\n",
    "    \n",
    "    flush()\n",
    "    return best_score, tabular_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360e698",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train all folds\n",
    "fold_scores = []\n",
    "for fold in range(cfg.n_folds):\n",
    "    score, _ = train_biomass_fold(fold, train_wide, cfg)\n",
    "    fold_scores.append(score)\n",
    "    print(f\"Fold {fold} Best CV: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä Overall CV: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e1b63",
   "metadata": {},
   "source": [
    "## ========== PHASE 3: Inference =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734b5c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(models: List[nn.Module], loader: DataLoader, device: str) -> np.ndarray:\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Inference'):\n",
    "        if len(batch) == 3:\n",
    "            imgs, tabular, _ = batch\n",
    "            imgs = imgs.to(device)\n",
    "            tabular = tabular.to(device)\n",
    "        else:\n",
    "            imgs, tabular = batch[0].to(device), None\n",
    "        \n",
    "        batch_preds = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            _, full_pred = model(imgs, tabular)\n",
    "            batch_preds.append(full_pred.cpu().numpy())\n",
    "        \n",
    "        avg_pred = np.mean(batch_preds, axis=0)\n",
    "        all_preds.append(avg_pred)\n",
    "    \n",
    "    return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\n",
    "models = []\n",
    "tabular_scaler = None\n",
    "\n",
    "for fold in range(cfg.n_folds):\n",
    "    ckpt_path = cfg.OUTPUT_DIR / f'biomass_fold{fold}.pt'\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "        \n",
    "        if fold == 0:\n",
    "            tabular_scaler = ckpt['tabular_scaler']\n",
    "        \n",
    "        model = BiomassModelWithTabular(cfg.backbone, n_tabular=2, pretrained=False).to(cfg.device)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        print(f\"‚úì Loaded fold {fold} (CV: {ckpt['score']:.4f})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b9c04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test dataset with pseudo tabular\n",
    "test_dataset = BiomassDatasetWithTabular(\n",
    "    test_wide, cfg, get_transforms('val', cfg.input_size), 'test', tabular_scaler\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "# Inference\n",
    "preds = inference(models, test_loader, cfg.device)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "def melt_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path', value_vars=TARGET_ORDER,\n",
    "        var_name='target_name', value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'target']]\n",
    "\n",
    "test_wide[TARGET_ORDER] = preds\n",
    "test_wide[TARGET_ORDER] = test_wide[TARGET_ORDER].clip(lower=0)\n",
    "\n",
    "submission = melt_table(test_wide)\n",
    "submission.to_csv(cfg.OUTPUT_DIR / 'submission.csv', index=False)\n",
    "\n",
    "# Verify physics constraints\n",
    "test_gdm = test_wide['Dry_Green_g'] + test_wide['Dry_Clover_g']\n",
    "test_total = test_gdm + test_wide['Dry_Dead_g']\n",
    "gdm_match = np.allclose(test_wide['GDM_g'], test_gdm)\n",
    "total_match = np.allclose(test_wide['Dry_Total_g'], test_total)\n",
    "\n",
    "print(f\"\\n‚úì Physics constraint check:\")\n",
    "print(f\"  GDM = Green + Clover: {gdm_match}\")\n",
    "print(f\"  Total = GDM + Dead: {total_match}\")\n",
    "\n",
    "print(f\"\\nüìÑ Submission saved: {len(submission)} rows\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "üèÜ Pseudo-Labeling Pipeline Complete!\n",
    "{'='*60}\n",
    "\n",
    "Pipeline:\n",
    "1. Phase 1: Image ‚Üí NDVI/Height predictor trained\n",
    "2. Phase 1.5: Pseudo tabular generated for test\n",
    "3. Phase 2: {cfg.n_folds}-Fold Biomass models trained\n",
    "4. Phase 3: Ensemble inference done\n",
    "\n",
    "CV Score: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\n",
    "Output: {cfg.OUTPUT_DIR / 'submission.csv'}\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
