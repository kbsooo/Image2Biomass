{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30cad8e",
   "metadata": {},
   "source": [
    "# üèÜ Physics-Constrained Biomass Prediction\n",
    "\n",
    "**Breakthrough Ideas:**\n",
    "1. Physics-Constrained Head: Only predict 3 targets (Green, Clover, Dead)\n",
    "   - GDM = Green + Clover (calculated)\n",
    "   - Total = GDM + Dead (calculated)\n",
    "2. Multi-Modal: Image + Tabular features (NDVI, Height)\n",
    "3. Pretrained EfficientNet-B4 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ec519",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346f7e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52be11",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4685aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # === Kaggle Paths ===\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "    \n",
    "    # Pretrained weights path (Kaggle Dataset)\n",
    "    # üìå Ïù¥ Í≤ΩÎ°úÎäî Î≥∏Ïù∏Ïùò Dataset Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî!\n",
    "    WEIGHTS_PATH = Path(\"/kaggle/input/pretrained-weights-biomass\")\n",
    "    \n",
    "    # === Model ===\n",
    "    backbone = \"efficientnet_b4\"  # ÎòêÎäî \"dinov2_vitb14\"\n",
    "    input_size = 384\n",
    "    \n",
    "    # === Training ===\n",
    "    n_folds = 5\n",
    "    epochs = 15\n",
    "    # GPU Í∞úÏàòÏóê Îî∞Îùº batch_size ÏûêÎèô Ï°∞Ï†ï\n",
    "    n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    batch_size = 8 * n_gpus  # T4 2Í∞úÎ©¥ 16\n",
    "    lr = 1e-4 * n_gpus  # Linear scaling rule\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # === Multi-Modal ===\n",
    "    # ‚ö†Ô∏è Test setÏóê tabular featuresÍ∞Ä ÏóÜÏúºÎØÄÎ°ú Ïù¥ÎØ∏ÏßÄÎßå ÏÇ¨Ïö©!\n",
    "    use_tabular = False\n",
    "    tabular_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "    \n",
    "    # === Misc ===\n",
    "    seed = 42\n",
    "    # Kaggle ÌôòÍ≤ΩÏóêÏÑú multiprocessing Ïò§Î•ò Î∞©ÏßÄÎ•º ÏúÑÌï¥ num_workers=0\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # === Targets (Î¨ºÎ¶¨Ï†Å Ï†úÏïΩ Ï°∞Í±¥ Í∏∞Î∞ò) ===\n",
    "    # ÏòàÏ∏°Ìï† ÎèÖÎ¶Ω Î≥ÄÏàò: Green, Clover, Dead\n",
    "    # Í≥ÑÏÇ∞ÎêòÎäî Î≥ÄÏàò: GDM = Green + Clover, Total = GDM + Dead\n",
    "    independent_targets = ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']\n",
    "    all_targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "cfg = CFG()\n",
    "cfg.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Use tabular: {cfg.use_tabular}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d4f5d",
   "metadata": {},
   "source": [
    "## Competition Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec940fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,  # Ïù¥Í≤ÉÏù¥ 50%!\n",
    "}\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "def competition_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Globally weighted R¬≤ (ÎåÄÌöå ÌèâÍ∞Ä ÏßÄÌëú)\"\"\"\n",
    "    weights = np.array([TARGET_WEIGHTS[t] for t in TARGET_ORDER])\n",
    "    \n",
    "    # Weighted mean\n",
    "    y_weighted_mean = sum(y_true[:, i].mean() * weights[i] for i in range(5))\n",
    "    \n",
    "    # SS_res and SS_tot\n",
    "    ss_res = sum(((y_true[:, i] - y_pred[:, i]) ** 2).mean() * weights[i] for i in range(5))\n",
    "    ss_tot = sum(((y_true[:, i] - y_weighted_mean) ** 2).mean() * weights[i] for i in range(5))\n",
    "    \n",
    "    return 1 - ss_res / (ss_tot + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340754ad",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4720bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"CSVÎ•º wide formatÏúºÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    if 'target' in df.columns:\n",
    "        # Train data: pivot to wide\n",
    "        df_wide = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        # Test data\n",
    "        df['target'] = 0\n",
    "        cols = ['image_path']\n",
    "        # Check if tabular columns exist\n",
    "        for col in ['Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']:\n",
    "            if col in df.columns:\n",
    "                cols.append(col)\n",
    "        \n",
    "        df_wide = df.drop_duplicates(subset=['image_path'])[cols].reset_index(drop=True)\n",
    "        \n",
    "        # Add dummy targets\n",
    "        for t in TARGET_ORDER:\n",
    "            df_wide[t] = 0.0\n",
    "    \n",
    "    return df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d13e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(cfg.DATA_PATH / \"train.csv\")\n",
    "train_wide = prepare_data(train_df, is_train=True)\n",
    "train_wide['image_id'] = train_wide['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# KFold split\n",
    "kf = KFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "train_wide['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(kf.split(train_wide)):\n",
    "    train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Train data shape: {train_wide.shape}\")\n",
    "print(f\"Columns: {train_wide.columns.tolist()}\")\n",
    "\n",
    "# Check tabular features\n",
    "if cfg.use_tabular:\n",
    "    for col in cfg.tabular_cols:\n",
    "        if col in train_wide.columns:\n",
    "            print(f\"  {col}: min={train_wide[col].min():.2f}, max={train_wide[col].max():.2f}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è {col} not found!\")\n",
    "            cfg.use_tabular = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305ed74",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b40f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode: str = 'train', size: int = 384) -> A.Compose:\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            # ÎèÑÎ©îÏù∏ ÌäπÌôî augmentation\n",
    "            A.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.3,\n",
    "                hue=0.05,\n",
    "                p=0.7\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43171f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760c910",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        cfg, \n",
    "        transforms=None, \n",
    "        mode: str = 'train',\n",
    "        tabular_scaler: Optional[StandardScaler] = None\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.tabular_scaler = tabular_scaler\n",
    "        \n",
    "        # Tabular features\n",
    "        self.use_tabular = cfg.use_tabular and all(col in df.columns for col in cfg.tabular_cols)\n",
    "        if self.use_tabular:\n",
    "            tabular_data = df[cfg.tabular_cols].values.astype(np.float32)\n",
    "            if self.tabular_scaler is not None:\n",
    "                if mode == 'train':\n",
    "                    self.tabular_data = self.tabular_scaler.fit_transform(tabular_data)\n",
    "                else:\n",
    "                    self.tabular_data = self.tabular_scaler.transform(tabular_data)\n",
    "            else:\n",
    "                self.tabular_data = tabular_data\n",
    "        else:\n",
    "            self.tabular_data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        # Targets (ÎèÖÎ¶Ω Î≥ÄÏàòÎßå: Green, Clover, Dead)\n",
    "        # ÏàúÏÑú: [Green, Clover, Dead]\n",
    "        targets = torch.tensor([\n",
    "            row['Dry_Green_g'],\n",
    "            row['Dry_Clover_g'],\n",
    "            row['Dry_Dead_g']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        # Tabular features\n",
    "        if self.use_tabular and self.tabular_data is not None:\n",
    "            tabular = torch.tensor(self.tabular_data[idx], dtype=torch.float32)\n",
    "            return img, tabular, targets\n",
    "        else:\n",
    "            return img, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907f668",
   "metadata": {},
   "source": [
    "## üîë Physics-Constrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25388c9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhysicsConstrainedHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Î¨ºÎ¶¨Ï†Å Ï†úÏïΩ Ï°∞Í±¥ÏùÑ ÎßåÏ°±ÌïòÎäî ÏòàÏ∏° Ìó§Îìú\n",
    "    \n",
    "    ÎèÖÎ¶Ω Î≥ÄÏàò: Green, Clover, Dead (3Í∞ú)\n",
    "    Ï¢ÖÏÜç Î≥ÄÏàò: GDM = Green + Clover, Total = GDM + Dead\n",
    "    \n",
    "    ‚û°Ô∏è 5Í∞ú ÌÉÄÍ≤ü Î™®Îëê Î¨ºÎ¶¨Ï†ÅÏúºÎ°ú ÏùºÍ¥ÄÏÑ± ÏûàÏùå!\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, hidden_dim: int = 256, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3Í∞ú ÎèÖÎ¶Ω Î≥ÄÏàò ÏòàÏ∏°\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 3)  # Green, Clover, Dead\n",
    "        )\n",
    "        \n",
    "        # Softplus for non-negative outputs\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            independent: [B, 3] - Green, Clover, Dead\n",
    "            full: [B, 5] - Green, Dead, Clover, GDM, Total (ÎåÄÌöå ÏàúÏÑú)\n",
    "        \"\"\"\n",
    "        raw = self.head(x)\n",
    "        independent = self.softplus(raw)  # ÎπÑÏùåÏàò Î≥¥Ïû•\n",
    "        \n",
    "        green = independent[:, 0:1]\n",
    "        clover = independent[:, 1:2]\n",
    "        dead = independent[:, 2:3]\n",
    "        \n",
    "        # Î¨ºÎ¶¨ Î≤ïÏπô Ï†ÅÏö© (Hard Constraint)\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        \n",
    "        # ÎåÄÌöå ÏàúÏÑú: [Green, Dead, Clover, GDM, Total]\n",
    "        full = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "        \n",
    "        return independent, full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fefa9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiModalBiomassModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Modal Model: Image + Tabular\n",
    "    \n",
    "    Features:\n",
    "    1. Pretrained CNN backbone\n",
    "    2. Tabular feature encoder with FiLM conditioning\n",
    "    3. Physics-Constrained prediction head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        backbone_name: str = \"efficientnet_b4\",\n",
    "        n_tabular: int = 2,\n",
    "        use_tabular: bool = True,\n",
    "        dropout: float = 0.3,\n",
    "        pretrained: bool = True,\n",
    "        weights_path: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_tabular = use_tabular\n",
    "        \n",
    "        # === Image Backbone ===\n",
    "        if pretrained and weights_path and Path(weights_path).exists():\n",
    "            # Load from local weights\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "            weights = torch.load(weights_path, weights_only=True)\n",
    "            # Remove classifier weights if present\n",
    "            weights = {k: v for k, v in weights.items() if not k.startswith('classifier')}\n",
    "            self.backbone.load_state_dict(weights, strict=False)\n",
    "            print(f\"‚úì Loaded pretrained weights from {weights_path}\")\n",
    "        elif pretrained:\n",
    "            # Try to load from timm (requires internet)\n",
    "            try:\n",
    "                self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
    "                print(\"‚úì Loaded pretrained weights from timm\")\n",
    "            except:\n",
    "                self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "                print(\"‚ö†Ô∏è Using random initialization (no pretrained weights)\")\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "        \n",
    "        self.feat_dim = self.backbone.num_features\n",
    "        \n",
    "        # === Tabular Encoder (FiLM conditioning) ===\n",
    "        if use_tabular:\n",
    "            self.tabular_encoder = nn.Sequential(\n",
    "                nn.Linear(n_tabular, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            # FiLM: Feature-wise Linear Modulation\n",
    "            self.film_gamma = nn.Linear(128, self.feat_dim)\n",
    "            self.film_beta = nn.Linear(128, self.feat_dim)\n",
    "        \n",
    "        # === Physics-Constrained Head ===\n",
    "        self.head = PhysicsConstrainedHead(\n",
    "            in_features=self.feat_dim, \n",
    "            hidden_dim=256, \n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        image: torch.Tensor, \n",
    "        tabular: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: [B, C, H, W]\n",
    "            tabular: [B, n_tabular] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            independent: [B, 3] - Green, Clover, Dead\n",
    "            full: [B, 5] - All 5 targets\n",
    "        \"\"\"\n",
    "        # Image features\n",
    "        img_feat = self.backbone(image)  # [B, feat_dim]\n",
    "        \n",
    "        # Tabular conditioning (FiLM)\n",
    "        if self.use_tabular and tabular is not None:\n",
    "            tab_feat = self.tabular_encoder(tabular)  # [B, 128]\n",
    "            gamma = self.film_gamma(tab_feat)  # [B, feat_dim]\n",
    "            beta = self.film_beta(tab_feat)    # [B, feat_dim]\n",
    "            \n",
    "            # FiLM modulation\n",
    "            img_feat = img_feat * (1 + gamma) + beta\n",
    "        \n",
    "        # Physics-Constrained prediction\n",
    "        independent, full = self.head(img_feat)\n",
    "        \n",
    "        return independent, full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42a8e5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bc75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module, \n",
    "    loader: DataLoader, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: str,\n",
    "    use_tabular: bool = False\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Train', leave=False):\n",
    "        if use_tabular:\n",
    "            imgs, tabular, targets = batch\n",
    "            imgs = imgs.to(device)\n",
    "            tabular = tabular.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            independent, _ = model(imgs, tabular)\n",
    "        else:\n",
    "            imgs, targets = batch\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            independent, _ = model(imgs)\n",
    "        \n",
    "        # MSE loss on independent variables\n",
    "        loss = F.mse_loss(independent, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: nn.Module, \n",
    "    loader: DataLoader, \n",
    "    device: str,\n",
    "    use_tabular: bool = False\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Valid', leave=False):\n",
    "        if use_tabular:\n",
    "            imgs, tabular, targets = batch\n",
    "            imgs = imgs.to(device)\n",
    "            tabular = tabular.to(device)\n",
    "            \n",
    "            _, full_pred = model(imgs, tabular)\n",
    "        else:\n",
    "            imgs, targets = batch\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            _, full_pred = model(imgs)\n",
    "        \n",
    "        all_preds.append(full_pred.cpu().numpy())\n",
    "        \n",
    "        # Reconstruct full targets from independent\n",
    "        green = targets[:, 0:1]\n",
    "        clover = targets[:, 1:2]\n",
    "        dead = targets[:, 2:3]\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        full_targets = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "        all_targets.append(full_targets.numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Competition metric\n",
    "    cv_score = competition_metric(all_targets, all_preds)\n",
    "    \n",
    "    # MSE for logging\n",
    "    mse = np.mean((all_preds - all_targets) ** 2)\n",
    "    \n",
    "    return mse, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561df37f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_fold(fold: int, train_df: pd.DataFrame, cfg) -> float:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Training Fold {fold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split\n",
    "    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "    \n",
    "    # Tabular scaler\n",
    "    tabular_scaler = StandardScaler() if cfg.use_tabular else None\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = BiomassDataset(\n",
    "        train_data, cfg, \n",
    "        get_transforms('train', cfg.input_size), \n",
    "        'train',\n",
    "        tabular_scaler\n",
    "    )\n",
    "    val_dataset = BiomassDataset(\n",
    "        val_data, cfg, \n",
    "        get_transforms('val', cfg.input_size), \n",
    "        'val',\n",
    "        tabular_scaler\n",
    "    )\n",
    "    \n",
    "    # Check if tabular is available\n",
    "    use_tabular = train_dataset.use_tabular\n",
    "    print(f\"Using tabular features: {use_tabular}\")\n",
    "    \n",
    "    # Loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
    "        num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=cfg.batch_size * 2, shuffle=False,\n",
    "        num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    weights_path = None\n",
    "    if cfg.WEIGHTS_PATH.exists():\n",
    "        weights_path = str(cfg.WEIGHTS_PATH / cfg.backbone / f\"{cfg.backbone}.pth\")\n",
    "    \n",
    "    model = MultiModalBiomassModel(\n",
    "        backbone_name=cfg.backbone,\n",
    "        n_tabular=len(cfg.tabular_cols),\n",
    "        use_tabular=use_tabular,\n",
    "        dropout=0.3,\n",
    "        pretrained=True,\n",
    "        weights_path=weights_path\n",
    "    )\n",
    "    \n",
    "    # Multi-GPU support\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üöÄ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(cfg.device)\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    \n",
    "    # Training loop\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, cfg.device, use_tabular\n",
    "        )\n",
    "        val_mse, cv_score = validate(model, val_loader, cfg.device, use_tabular)\n",
    "        scheduler.step()\n",
    "        \n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | LR: {lr:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val MSE: {val_mse:.4f} | CV: {cv_score:.4f}\")\n",
    "        \n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            # Save model (handle DataParallel)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'fold': fold,\n",
    "                'score': best_score,\n",
    "                'tabular_scaler': tabular_scaler,\n",
    "                'use_tabular': use_tabular\n",
    "            }, cfg.OUTPUT_DIR / f'best_model_fold{fold}.pt')\n",
    "            print(f\"  ‚úì New best! Saved.\")\n",
    "    \n",
    "    flush()\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff4783",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train all folds\n",
    "fold_scores = []\n",
    "for fold in range(cfg.n_folds):\n",
    "    score = train_fold(fold, train_wide, cfg)\n",
    "    fold_scores.append(score)\n",
    "    print(f\"Fold {fold} Best CV: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä Overall CV: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cf8e0",
   "metadata": {},
   "source": [
    "## Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4d9b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(\n",
    "    models: list, \n",
    "    loader: DataLoader, \n",
    "    device: str,\n",
    "    use_tabular: bool = False\n",
    ") -> np.ndarray:\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Inference'):\n",
    "        # ÎèôÏ†ÅÏúºÎ°ú batch unpacking (tabular Ïú†Î¨¥Ïóê Îî∞Îùº 2Í∞ú ÎòêÎäî 3Í∞ú)\n",
    "        if len(batch) == 3:\n",
    "            imgs, tabular, _ = batch\n",
    "            imgs = imgs.to(device)\n",
    "            tabular = tabular.to(device)\n",
    "            has_tabular = True\n",
    "        else:\n",
    "            imgs, _ = batch\n",
    "            imgs = imgs.to(device)\n",
    "            tabular = None\n",
    "            has_tabular = False\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        batch_preds = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            if has_tabular and use_tabular:\n",
    "                _, full_pred = model(imgs, tabular)\n",
    "            else:\n",
    "                _, full_pred = model(imgs)\n",
    "            batch_preds.append(full_pred.cpu().numpy())\n",
    "        \n",
    "        # Average\n",
    "        avg_pred = np.mean(batch_preds, axis=0)\n",
    "        all_preds.append(avg_pred)\n",
    "    \n",
    "    return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_wide = prepare_data(test_df, is_train=False)\n",
    "\n",
    "print(f\"Test data: {len(test_wide)} images\")\n",
    "print(f\"Test columns: {test_wide.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all fold models\n",
    "models = []\n",
    "use_tabular = False\n",
    "tabular_scaler = None\n",
    "\n",
    "for fold in range(cfg.n_folds):\n",
    "    ckpt_path = cfg.OUTPUT_DIR / f'best_model_fold{fold}.pt'\n",
    "    if ckpt_path.exists():\n",
    "        ckpt = torch.load(ckpt_path, weights_only=False)\n",
    "        \n",
    "        # Get config from first checkpoint\n",
    "        if fold == 0:\n",
    "            use_tabular = ckpt.get('use_tabular', False)\n",
    "            tabular_scaler = ckpt.get('tabular_scaler', None)\n",
    "        \n",
    "        model = MultiModalBiomassModel(\n",
    "            backbone_name=cfg.backbone,\n",
    "            n_tabular=len(cfg.tabular_cols),\n",
    "            use_tabular=use_tabular,\n",
    "            pretrained=False\n",
    "        ).to(cfg.device)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        print(f\"‚úì Loaded fold {fold} (CV: {ckpt['score']:.4f})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77352fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = BiomassDataset(\n",
    "    test_wide, cfg, \n",
    "    get_transforms('val', cfg.input_size), \n",
    "    'test',\n",
    "    tabular_scaler\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=cfg.num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "# Inference\n",
    "preds = inference(models, test_loader, cfg.device, use_tabular)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "def melt_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path', value_vars=TARGET_ORDER,\n",
    "        var_name='target_name', value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]\n",
    "\n",
    "# Apply predictions\n",
    "test_wide[TARGET_ORDER] = preds\n",
    "\n",
    "# Clip to non-negative (should already be non-negative due to Softplus)\n",
    "test_wide[TARGET_ORDER] = test_wide[TARGET_ORDER].clip(lower=0)\n",
    "\n",
    "submission = melt_table(test_wide)\n",
    "submission = submission[['sample_id', 'target']]\n",
    "\n",
    "# Verify physics constraints\n",
    "test_gdm = test_wide['Dry_Green_g'] + test_wide['Dry_Clover_g']\n",
    "test_total = test_gdm + test_wide['Dry_Dead_g']\n",
    "gdm_match = np.allclose(test_wide['GDM_g'], test_gdm)\n",
    "total_match = np.allclose(test_wide['Dry_Total_g'], test_total)\n",
    "print(f\"\\n‚úì Physics constraint check:\")\n",
    "print(f\"  GDM = Green + Clover: {gdm_match}\")\n",
    "print(f\"  Total = GDM + Dead: {total_match}\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(cfg.OUTPUT_DIR / 'submission.csv', index=False)\n",
    "print(f\"\\nüìÑ Submission saved: {len(submission)} rows\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"\\n=== Submission Verification ===\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "print(f\"Null values: {submission.isnull().sum().sum()}\")\n",
    "print(f\"Target range: [{submission['target'].min():.2f}, {submission['target'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8526cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "üèÜ Physics-Constrained Baseline Complete!\n",
    "{'='*60}\n",
    "\n",
    "Output: {cfg.OUTPUT_DIR / 'submission.csv'}\n",
    "CV Score: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\n",
    "\n",
    "Key Features:\n",
    "1. ‚úÖ Physics-Constrained Head (3 independent, 2 derived)\n",
    "2. ‚úÖ {cfg.n_folds}-Fold Cross-Validation\n",
    "3. ‚úÖ Pretrained {cfg.backbone} backbone\n",
    "4. ‚úÖ Multi-Modal (Image + Tabular): {use_tabular}\n",
    "5. ‚úÖ Domain-specific augmentations\n",
    "\n",
    "Next steps:\n",
    "- Try DINOv2 backbone for potentially higher performance\n",
    "- Add TTA (Test-Time Augmentation)\n",
    "- Experiment with different loss functions\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
