{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ed2f53",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - Simple Baseline for Submission\n",
    "\n",
    "**Goal**: Get a valid submission.csv first, then iterate\n",
    "\n",
    "**Strategy**:\n",
    "- EfficientNet-B0 (torchvision, no external weights needed)\n",
    "- Simple regression head\n",
    "- Fast training (5 epochs)\n",
    "- Single fold for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9bb4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Kaggle 커널 재시작 필요 시 이 셀만 먼저 실행 ===\n",
    "# 런타임 -> 세션 다시 시작 후 실행\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# PyTorch imports (커널 재시작 후 정상 작동)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2b83d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f41d5c",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # === Kaggle Paths ===\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "    # === Model ===\n",
    "    model_name = \"efficientnet_b0\"  # torchvision 기본 제공\n",
    "    input_size = 384  # EfficientNet-B0 default\n",
    "\n",
    "    # === Training ===\n",
    "    epochs = 10  # pretrained 없이는 더 많은 epoch 필요\n",
    "    batch_size = 8\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    # === Misc ===\n",
    "    seed = 42\n",
    "    num_workers = 2\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # === Targets ===\n",
    "    targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "cfg = CFG()\n",
    "cfg.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Data path: {cfg.DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fbba2",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf05232",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "def competition_metric(y_true, y_pred):\n",
    "    \"\"\"Globally weighted R².\"\"\"\n",
    "    weights = np.array([TARGET_WEIGHTS[t] for t in TARGET_ORDER])\n",
    "    y_weighted_mean = sum(y_true[:, i].mean() * weights[i] for i in range(5))\n",
    "    ss_res = sum(((y_true[:, i] - y_pred[:, i]) ** 2).mean() * weights[i] for i in range(5))\n",
    "    ss_tot = sum(((y_true[:, i] - y_weighted_mean) ** 2).mean() * weights[i] for i in range(5))\n",
    "    return 1 - ss_res / (ss_tot + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5504a2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9171d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pivot_table(df):\n",
    "    if 'target' in df.columns:\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        df_pt = pd.pivot_table(\n",
    "            df, values='target', index='image_path',\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    return df_pt\n",
    "\n",
    "def melt_table(df):\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path', value_vars=TARGET_ORDER,\n",
    "        var_name='target_name', value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'image_path', 'target_name', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dffaa5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_df = pd.read_csv(cfg.DATA_PATH / \"train.csv\")\n",
    "train_wide = pivot_table(train_df)\n",
    "train_wide['image_id'] = train_wide['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# Simple KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=cfg.seed)\n",
    "train_wide['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(kf.split(train_wide)):\n",
    "    train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Train data shape: {train_wide.shape}\")\n",
    "print(train_wide.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b3235",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264fb8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode='train', size=384):\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed322aca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, cfg, transforms=None, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.tensor([row[t] for t in TARGET_ORDER], dtype=torch.float32)\n",
    "\n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224f4db",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35504630",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-B0 + Simple regression head.\n",
    "    Internet OFF 환경에서는 pretrained=False 사용.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_targets=5, pretrained=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # EfficientNet-B0 backbone (pretrained=False for offline mode)\n",
    "        weights = None  # Internet OFF에서는 weights 다운로드 불가\n",
    "        self.backbone = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "        # Get feature dimension\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Replace classifier with regression head\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "\n",
    "        # Softplus for non-negative outputs\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.softplus(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec756de3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7989a4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc='Train', leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc='Valid', leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    cv_score = competition_metric(all_targets, all_preds)\n",
    "\n",
    "    return total_loss / len(loader), cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e786ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_fold(fold, train_df, cfg):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Split\n",
    "    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = BiomassDataset(train_data, cfg, get_transforms('train', cfg.input_size), 'train')\n",
    "    val_dataset = BiomassDataset(val_data, cfg, get_transforms('val', cfg.input_size), 'val')\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size * 2, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    model = SimpleBaseline(num_targets=5, pretrained=True).to(cfg.device)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "\n",
    "    # Loss - weighted MSE\n",
    "    weights = torch.tensor([TARGET_WEIGHTS[t] for t in TARGET_ORDER]).to(cfg.device)\n",
    "    def weighted_mse(pred, target):\n",
    "        return ((pred - target) ** 2 * weights).mean()\n",
    "\n",
    "    # Training loop\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, weighted_mse, cfg.device)\n",
    "        val_loss, cv_score = validate(model, val_loader, weighted_mse, cfg.device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | CV: {cv_score:.4f}\")\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            torch.save(model.state_dict(), cfg.OUTPUT_DIR / f'best_model_fold{fold}.pt')\n",
    "            print(f\"  -> New best! Saved.\")\n",
    "\n",
    "    flush()\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f58351",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train only fold 0 for speed\n",
    "best_score = train_fold(0, train_wide, cfg)\n",
    "print(f\"\\nBest CV Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3abdb",
   "metadata": {},
   "source": [
    "## Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a68e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    for imgs, _ in tqdm(loader, desc='Inference'):\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_wide = test_df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "test_wide['image_id'] = test_wide['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# Add dummy targets\n",
    "for t in TARGET_ORDER:\n",
    "    if t not in test_wide.columns:\n",
    "        test_wide[t] = 0.0\n",
    "\n",
    "print(f\"Test data: {len(test_wide)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72473ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = SimpleBaseline(num_targets=5, pretrained=False).to(cfg.device)\n",
    "model.load_state_dict(torch.load(cfg.OUTPUT_DIR / 'best_model_fold0.pt', weights_only=True))\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = BiomassDataset(test_wide, cfg, get_transforms('val', cfg.input_size), 'test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                         num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "# Inference\n",
    "preds = inference(model, test_loader, cfg.device)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "test_wide[TARGET_ORDER] = preds\n",
    "submission = melt_table(test_wide)\n",
    "submission = submission[['sample_id', 'target']]\n",
    "\n",
    "# Clip to non-negative\n",
    "submission['target'] = submission['target'].clip(lower=0)\n",
    "\n",
    "# Save\n",
    "submission.to_csv(cfg.OUTPUT_DIR / 'submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved: {len(submission)} rows\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission format\n",
    "print(\"\\n=== Submission Verification ===\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "print(f\"Null values: {submission.isnull().sum().sum()}\")\n",
    "print(f\"Target range: [{submission['target'].min():.2f}, {submission['target'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c9a69",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "submission.csv가 `/kaggle/working/submission.csv`에 저장되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21318cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*50}\n",
    "Baseline Complete!\n",
    "{'='*50}\n",
    "\n",
    "Output: {cfg.OUTPUT_DIR / 'submission.csv'}\n",
    "CV Score: {best_score:.4f}\n",
    "\n",
    "Next steps:\n",
    "1. Submit this notebook to verify pipeline\n",
    "2. Improve model architecture\n",
    "3. Add more epochs, folds, TTA\n",
    "{'='*50}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
