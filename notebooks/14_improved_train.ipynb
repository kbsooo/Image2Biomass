{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c149958",
   "metadata": {},
   "source": [
    "# üèÜ Improved DINOv3 Training Pipeline v2\n",
    "\n",
    "**Î™©Ìëú**: CV 0.70+ Îã¨ÏÑ±\n",
    "\n",
    "**ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠**:\n",
    "1. Í≤ÄÏ¶ùÎêú baseline Í∏∞Î∞ò (head=128, dropout=0.1)\n",
    "2. Î≥¥ÏàòÏ†Å augmentation (ÎÜçÏóÖ Ïù¥ÎØ∏ÏßÄ ÌäπÏÑ± Í≥†Î†§)\n",
    "3. Huber Loss (outlier robust)\n",
    "4. EMA (Exponential Moving Average)\n",
    "5. MixUp regularization\n",
    "6. Log1p target transformation\n",
    "7. Zero-Inflated Clover Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f40ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import timm\n",
    "import torchvision.transforms.v2 as T\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d64fc",
   "metadata": {},
   "source": [
    "## üîê Step 1: Google Drive Mount (Colab Only)\n",
    "**Ï§ëÏöî**: Ïù¥ ÏÖÄÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏó¨ Drive Í∂åÌïúÏùÑ ÏäπÏù∏ÌïòÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDRIVE_SAVE_PATH = None\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    GDRIVE_SAVE_PATH = Path('/content/drive/MyDrive/kaggle_models/csiro_biomass_v2')\n",
    "    GDRIVE_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úì Google Drive mounted: {GDRIVE_SAVE_PATH}\")\n",
    "except ImportError:\n",
    "    print(\"Not in Colab - Google Drive skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c635d58",
   "metadata": {},
   "source": [
    "## üîë Step 2: Kaggle Login (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c53a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "IS_KAGGLE = Path(\"/kaggle/input/csiro-biomass\").exists()\n",
    "\n",
    "if not IS_KAGGLE:\n",
    "    print(\"üü¢ Colab ÌôòÍ≤Ω - Kaggle Î°úÍ∑∏Ïù∏ ÌïÑÏöî\")\n",
    "    kagglehub.login()\n",
    "else:\n",
    "    print(\"üîµ Kaggle ÌôòÍ≤Ω\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3c3e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee196b7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f92971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # === Paths ===\n",
    "    DATA_PATH = None\n",
    "    OUTPUT_DIR = None\n",
    "    WEIGHTS_PATH = None\n",
    "\n",
    "    # === Model ===\n",
    "    model_name = \"vit_large_patch16_dinov3_qkvb.lvd1689m\"\n",
    "    backbone_dim = 1024\n",
    "    img_size = (512, 512)  # patch16 Î™®Îç∏Ïù¥ÎØÄÎ°ú 16Ïùò Î∞∞Ïàò ÌïÑÏöî\n",
    "\n",
    "    # === Head Architecture ===\n",
    "    head_hidden_dim = 128  # 8ÏùÄ Ïã§Ìå®, 256ÏùÄ CV 0.6 ‚Üí Ï§ëÍ∞ÑÍ∞í ÏãúÎèÑ\n",
    "    use_zero_inflated_clover = True\n",
    "\n",
    "    # === Training ===\n",
    "    n_folds = 5\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    accumulation_steps = 2  # effective batch = 16\n",
    "    lr = 5e-5\n",
    "    backbone_lr_mult = 0.1\n",
    "    weight_decay = 1e-4\n",
    "    dropout = 0.1\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # === Loss ===\n",
    "    use_huber_loss = True\n",
    "    huber_delta = 1.0\n",
    "\n",
    "    # === EMA ===\n",
    "    use_ema = True\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    # === Gradient ===\n",
    "    gradient_clip = 1.0\n",
    "\n",
    "    # === Augmentation ===\n",
    "    use_mixup = True\n",
    "    mixup_alpha = 0.2\n",
    "\n",
    "    # === Target Transform ===\n",
    "    use_log1p = True\n",
    "\n",
    "    # === Other ===\n",
    "    seed = 42\n",
    "    num_workers = 4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8710600",
   "metadata": {},
   "source": [
    "## üì• Step 3: Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    cfg.DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    cfg.WEIGHTS_PATH = Path(\"/kaggle/input/pretrained-weights-biomass/dinov3_large/dinov3_large\")\n",
    "    cfg.OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "else:\n",
    "    print(\"Downloading data via kagglehub...\")\n",
    "    csiro_path = kagglehub.competition_download('csiro-biomass')\n",
    "    weights_path = kagglehub.dataset_download('kbsooo/pretrained-weights-biomass')\n",
    "\n",
    "    cfg.DATA_PATH = Path(csiro_path)\n",
    "    cfg.WEIGHTS_PATH = Path(weights_path) / \"dinov3_large\" / \"dinov3_large\"\n",
    "    cfg.OUTPUT_DIR = Path(\"/content/output\")\n",
    "\n",
    "cfg.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data: {cfg.DATA_PATH}\")\n",
    "print(f\"Weights: {cfg.WEIGHTS_PATH}\")\n",
    "print(f\"Output: {cfg.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0db28",
   "metadata": {},
   "source": [
    "## üìä Competition Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541d546",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2, 'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "def competition_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Weighted R¬≤ score\"\"\"\n",
    "    weighted_r2 = 0.0\n",
    "    for i, target in enumerate(TARGET_ORDER):\n",
    "        weight = TARGET_WEIGHTS[target]\n",
    "        ss_res = np.sum((y_true[:, i] - y_pred[:, i]) ** 2)\n",
    "        ss_tot = np.sum((y_true[:, i] - np.mean(y_true[:, i])) ** 2)\n",
    "        r2 = 1 - ss_res / (ss_tot + 1e-8)\n",
    "        weighted_r2 += weight * r2\n",
    "    return weighted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079bb53",
   "metadata": {},
   "source": [
    "## üìÅ Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c390d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pivot = df.pivot_table(\n",
    "        index=['image_path', 'State', 'Species', 'Sampling_Date', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "        columns='target_name',\n",
    "        values='target',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    pivot.columns.name = None\n",
    "    return pivot\n",
    "\n",
    "train_df = pd.read_csv(cfg.DATA_PATH / \"train.csv\")\n",
    "train_wide = prepare_data(train_df)\n",
    "train_wide['image_id'] = train_wide['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# Stratified Group KFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "train_wide['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(sgkf.split(\n",
    "    train_wide,\n",
    "    train_wide['State'],\n",
    "    groups=train_wide['image_id']\n",
    ")):\n",
    "    train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Train samples: {len(train_wide)}\")\n",
    "print(f\"Folds: {train_wide['fold'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Target Î∂ÑÌè¨ ÌôïÏù∏\n",
    "print(f\"\\nTarget statistics:\")\n",
    "for col in ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']:\n",
    "    zeros = (train_wide[col] == 0).sum()\n",
    "    print(f\"  {col}: mean={train_wide[col].mean():.2f}, zeros={zeros} ({zeros/len(train_wide)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2c318",
   "metadata": {},
   "source": [
    "## üé® Data Augmentation\n",
    "\n",
    "**Ï†ÑÎûµ**: ÎÜçÏóÖ Ïù¥ÎØ∏ÏßÄ ÌäπÏÑ±ÏùÑ Í≥†Î†§Ìïú Î≥¥ÏàòÏ†Å augmentation\n",
    "- ÏÉâÏÉÅ Î≥ÄÌôòÏùÄ ÏµúÏÜåÌôî (ÎÖπÏÉâ/Í∞àÏÉâÏù¥ Î∞îÏù¥Ïò§Îß§Ïä§ ÏòàÏ∏°Ïóê Ï§ëÏöî)\n",
    "- Í∏∞ÌïòÌïôÏ†Å Î≥ÄÌôò ÏúÑÏ£º (flip, rotation)\n",
    "- MixUpÏúºÎ°ú regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbd7f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_train_transforms(cfg):\n",
    "    \"\"\"\n",
    "    Î≥¥ÏàòÏ†Å augmentation:\n",
    "    - Flip: ÏãùÎ¨º Ïù¥ÎØ∏ÏßÄÏóê ÏïàÏ†Ñ\n",
    "    - ÏûëÏùÄ ÌöåÏ†Ñ: Ïπ¥Î©îÎùº Í∞ÅÎèÑ Î≥ÄÌôî ÏãúÎÆ¨Î†àÏù¥ÏÖò\n",
    "    - Í∞ÄÎ≤ºÏö¥ ÏÉâÏÉÅ Î≥ÄÌôò: Ï°∞Î™Ö Î≥ÄÌôî ÏãúÎÆ¨Î†àÏù¥ÏÖò (ÎÑàÎ¨¥ Í∞ïÌïòÎ©¥ ÏïàÎê®)\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize(cfg.img_size),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=10),  # ÏûëÏùÄ ÌöåÏ†ÑÎßå\n",
    "        T.ColorJitter(\n",
    "            brightness=0.1,  # ÏïΩÌïú Î∞ùÍ∏∞ Î≥ÄÌôî\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.02  # ÏÉâÏ°∞Îäî Í±∞Ïùò Ïïà Î∞îÍøà (ÎÖπÏÉâ/Í∞àÏÉâ Î≥¥Ï°¥)\n",
    "        ),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(cfg):\n",
    "    return T.Compose([\n",
    "        T.Resize(cfg.img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e5921",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    \"\"\"Left/Right split dataset with log1p transform\"\"\"\n",
    "    def __init__(self, df, cfg, transform=None, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img = Image.open(self.cfg.DATA_PATH / row['image_path']).convert('RGB')\n",
    "        width, height = img.size\n",
    "        mid = width // 2\n",
    "\n",
    "        left_img = img.crop((0, 0, mid, height))\n",
    "        right_img = img.crop((mid, 0, width, height))\n",
    "\n",
    "        if self.transform:\n",
    "            left_img = self.transform(left_img)\n",
    "            right_img = self.transform(right_img)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            targets = torch.tensor([\n",
    "                row['Dry_Green_g'],\n",
    "                row['Dry_Clover_g'],\n",
    "                row['Dry_Dead_g']\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "            if self.cfg.use_log1p:\n",
    "                targets = torch.log1p(targets)\n",
    "\n",
    "            return left_img, right_img, targets\n",
    "        else:\n",
    "            return left_img, right_img, row['image_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7dbfa1",
   "metadata": {},
   "source": [
    "## üîÄ MixUp (Simple Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ee485",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mixup_data(left, right, targets, alpha=0.2):\n",
    "    \"\"\"Simple MixUp for regression\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = left.size(0)\n",
    "    index = torch.randperm(batch_size, device=left.device)\n",
    "\n",
    "    mixed_left = lam * left + (1 - lam) * left[index]\n",
    "    mixed_right = lam * right + (1 - lam) * right[index]\n",
    "    mixed_targets = lam * targets + (1 - lam) * targets[index]\n",
    "\n",
    "    return mixed_left, mixed_right, mixed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6e13e",
   "metadata": {},
   "source": [
    "## üß† Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2ecdc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FiLM(nn.Module):\n",
    "    \"\"\"Feature-wise Linear Modulation\"\"\"\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim // 2, feat_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        out = self.mlp(context)\n",
    "        gamma, beta = torch.chunk(out, 2, dim=1)\n",
    "        return gamma, beta\n",
    "\n",
    "\n",
    "class ZeroInflatedHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Zero-Inflated Head for Clover (38% zeros)\n",
    "    Two-stage: (1) P(positive) (2) amount if positive\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        prob = torch.sigmoid(self.classifier(x))\n",
    "        amount = self.regressor(x)\n",
    "        return prob * amount\n",
    "\n",
    "\n",
    "class CSIROModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    DINOv3 + FiLM + Configurable Head\n",
    "\n",
    "    Changes from v1:\n",
    "    - Configurable head_hidden_dim\n",
    "    - Optional Zero-Inflated Clover head\n",
    "    - Simpler architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Backbone\n",
    "        weights_file = cfg.WEIGHTS_PATH / \"dinov3_vitl16_qkvb.pth\"\n",
    "        if weights_file.exists():\n",
    "            print(f\"Loading backbone from: {weights_file}\")\n",
    "            self.backbone = timm.create_model(\n",
    "                cfg.model_name, pretrained=False, num_classes=0, global_pool='avg'\n",
    "            )\n",
    "            state = torch.load(weights_file, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(state, strict=False)\n",
    "            print(\"‚úì Backbone loaded\")\n",
    "        else:\n",
    "            print(\"Loading backbone from timm (online)\")\n",
    "            self.backbone = timm.create_model(\n",
    "                cfg.model_name, pretrained=True, num_classes=0, global_pool='avg'\n",
    "            )\n",
    "\n",
    "        feat_dim = self.backbone.num_features\n",
    "        combined_dim = feat_dim * 2\n",
    "        hidden_dim = cfg.head_hidden_dim\n",
    "        dropout = cfg.dropout\n",
    "\n",
    "        # FiLM\n",
    "        self.film = FiLM(feat_dim)\n",
    "\n",
    "        # Heads\n",
    "        def make_head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(combined_dim, hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "\n",
    "        self.head_green = make_head()\n",
    "        self.head_dead = make_head()\n",
    "\n",
    "        if cfg.use_zero_inflated_clover:\n",
    "            self.head_clover = ZeroInflatedHead(combined_dim, hidden_dim, dropout)\n",
    "        else:\n",
    "            self.head_clover = make_head()\n",
    "\n",
    "        print(f\"Model: head_dim={hidden_dim}, dropout={dropout}, ZI_clover={cfg.use_zero_inflated_clover}\")\n",
    "\n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.backbone(left_img)\n",
    "        right_feat = self.backbone(right_img)\n",
    "\n",
    "        context = (left_feat + right_feat) / 2\n",
    "        gamma, beta = self.film(context)\n",
    "\n",
    "        left_mod = left_feat * (1 + gamma) + beta\n",
    "        right_mod = right_feat * (1 + gamma) + beta\n",
    "\n",
    "        combined = torch.cat([left_mod, right_mod], dim=1)\n",
    "\n",
    "        green = self.head_green(combined)\n",
    "        clover = self.head_clover(combined)\n",
    "        dead = self.head_dead(combined)\n",
    "\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "\n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2d2cf",
   "metadata": {},
   "source": [
    "## üìà EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f38a5c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model weights\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self._register()\n",
    "\n",
    "    def _register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = (\n",
    "                    self.decay * self.shadow[name] + (1 - self.decay) * param.data\n",
    "                )\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\"Apply EMA weights for evaluation\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"Restore original weights after evaluation\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8b27f",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085e2af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, scheduler, scaler, cfg, ema=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for step, (left, right, targets) in enumerate(pbar):\n",
    "        left = left.to(cfg.device)\n",
    "        right = right.to(cfg.device)\n",
    "        targets = targets.to(cfg.device)\n",
    "\n",
    "        # MixUp\n",
    "        if cfg.use_mixup and np.random.random() < 0.5:\n",
    "            left, right, targets = mixup_data(left, right, targets, cfg.mixup_alpha)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(left, right)\n",
    "            pred = outputs[:, [0, 2, 1]]  # [Green, Clover, Dead]\n",
    "\n",
    "            if cfg.use_huber_loss:\n",
    "                loss = F.huber_loss(pred, targets, delta=cfg.huber_delta)\n",
    "            else:\n",
    "                loss = F.mse_loss(pred, targets)\n",
    "\n",
    "            loss = loss / cfg.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % cfg.accumulation_steps == 0:\n",
    "            if cfg.gradient_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.update()\n",
    "\n",
    "        total_loss += loss.item() * cfg.accumulation_steps\n",
    "        pbar.set_postfix({'loss': f'{loss.item() * cfg.accumulation_steps:.4f}'})\n",
    "\n",
    "    # Flush remaining gradients\n",
    "    if (step + 1) % cfg.accumulation_steps != 0:\n",
    "        if cfg.gradient_clip > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        if ema is not None:\n",
    "            ema.update()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, cfg):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for left, right, targets in tqdm(loader, desc=\"Validating\"):\n",
    "        left = left.to(cfg.device)\n",
    "        right = right.to(cfg.device)\n",
    "\n",
    "        outputs = model(left, right)\n",
    "\n",
    "        if cfg.use_log1p:\n",
    "            outputs = torch.expm1(outputs)\n",
    "\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_targets.append(targets.numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "\n",
    "    if cfg.use_log1p:\n",
    "        targets = np.expm1(targets)\n",
    "\n",
    "    # Build full targets\n",
    "    full_targets = np.zeros((len(targets), 5))\n",
    "    full_targets[:, 0] = targets[:, 0]  # Green\n",
    "    full_targets[:, 1] = targets[:, 2]  # Dead\n",
    "    full_targets[:, 2] = targets[:, 1]  # Clover\n",
    "    full_targets[:, 3] = targets[:, 0] + targets[:, 1]  # GDM\n",
    "    full_targets[:, 4] = full_targets[:, 3] + targets[:, 2]  # Total\n",
    "\n",
    "    score = competition_metric(full_targets, preds)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb631212",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_fold(fold, train_df, cfg):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    train_data = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = BiomassDataset(train_data, cfg, get_train_transforms(cfg), 'train')\n",
    "    val_ds = BiomassDataset(val_data, cfg, get_val_transforms(cfg), 'train')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=cfg.batch_size,\n",
    "        shuffle=True, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=cfg.batch_size * 2,\n",
    "        shuffle=False, num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = CSIROModelV2(cfg).to(cfg.device)\n",
    "\n",
    "    # EMA\n",
    "    ema = EMA(model, cfg.ema_decay) if cfg.use_ema else None\n",
    "\n",
    "    # Optimizer\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "    head_params = (\n",
    "        list(model.head_green.parameters()) +\n",
    "        list(model.head_clover.parameters()) +\n",
    "        list(model.head_dead.parameters()) +\n",
    "        list(model.film.parameters())\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {'params': backbone_params, 'lr': cfg.lr * cfg.backbone_lr_mult},\n",
    "        {'params': head_params, 'lr': cfg.lr}\n",
    "    ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "    total_steps = len(train_loader) * cfg.epochs // cfg.accumulation_steps\n",
    "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    best_score = -float('inf')\n",
    "    best_epoch = 0\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, scaler, cfg, ema\n",
    "        )\n",
    "\n",
    "        # Validate with EMA weights\n",
    "        if ema is not None:\n",
    "            ema.apply_shadow()\n",
    "\n",
    "        val_score = validate(model, val_loader, cfg)\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.restore()\n",
    "\n",
    "        print(f\"Loss: {train_loss:.4f} | CV: {val_score:.4f}\")\n",
    "\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_epoch = epoch + 1\n",
    "            no_improve = 0\n",
    "\n",
    "            # Save with EMA weights\n",
    "            if ema is not None:\n",
    "                ema.apply_shadow()\n",
    "\n",
    "            torch.save(model.state_dict(), cfg.OUTPUT_DIR / f'model_fold{fold}.pth')\n",
    "\n",
    "            if ema is not None:\n",
    "                ema.restore()\n",
    "\n",
    "            print(f\"  ‚úì New best! Saved.\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nFold {fold} Best: {best_score:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "    # Backup to Google Drive\n",
    "    if GDRIVE_SAVE_PATH is not None:\n",
    "        src = cfg.OUTPUT_DIR / f'model_fold{fold}.pth'\n",
    "        if src.exists():\n",
    "            shutil.copy(src, GDRIVE_SAVE_PATH / f'model_fold{fold}.pth')\n",
    "            print(f\"  üìÅ Backed up to Drive\")\n",
    "\n",
    "    flush()\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297707c",
   "metadata": {},
   "source": [
    "## üöÄ Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aea926",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ TRAINING START\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Config: head={cfg.head_hidden_dim}, dropout={cfg.dropout}, lr={cfg.lr}\")\n",
    "    print(f\"        mixup={cfg.use_mixup}, huber={cfg.use_huber_loss}, ema={cfg.use_ema}\")\n",
    "    print(f\"        log1p={cfg.use_log1p}, ZI_clover={cfg.use_zero_inflated_clover}\")\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold in range(cfg.n_folds):\n",
    "        score = train_fold(fold, train_wide, cfg)\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Fold scores: {[f'{s:.4f}' for s in fold_scores]}\")\n",
    "    print(f\"Mean CV: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n",
    "\n",
    "    # Save to Google Drive\n",
    "    if GDRIVE_SAVE_PATH is not None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        final_path = GDRIVE_SAVE_PATH / f\"run_{timestamp}_cv{np.mean(fold_scores):.4f}\"\n",
    "        final_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for f in cfg.OUTPUT_DIR.glob(\"model_fold*.pth\"):\n",
    "            shutil.copy(f, final_path / f.name)\n",
    "\n",
    "        results = {\n",
    "            'fold_scores': fold_scores,\n",
    "            'mean_cv': float(np.mean(fold_scores)),\n",
    "            'std_cv': float(np.std(fold_scores)),\n",
    "            'config': {\n",
    "                'model_name': cfg.model_name,\n",
    "                'head_hidden_dim': cfg.head_hidden_dim,\n",
    "                'dropout': cfg.dropout,\n",
    "                'lr': cfg.lr,\n",
    "                'backbone_lr_mult': cfg.backbone_lr_mult,\n",
    "                'use_huber_loss': cfg.use_huber_loss,\n",
    "                'use_ema': cfg.use_ema,\n",
    "                'use_mixup': cfg.use_mixup,\n",
    "                'use_log1p': cfg.use_log1p,\n",
    "            }\n",
    "        }\n",
    "        with open(final_path / 'results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(f\"\\n‚úÖ Saved to: {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
