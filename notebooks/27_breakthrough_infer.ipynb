{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7e91c2",
   "metadata": {},
   "source": [
    "# üöÄ v27: Breakthrough Multi-Model Inference\n",
    "\n",
    "**Î™©Ï†Å**: v20, v22, v23, v25, v26 Î™®Îç∏ ÏµúÏ†Å Í≤∞Ìï©ÏúºÎ°ú 0.69 Î≤Ω ÎèåÌåå\n",
    "\n",
    "**Ï†ÑÎûµ**:\n",
    "1. Î™®Îì† Î™®Îç∏ ÏòàÏ∏° ÏàòÏßë\n",
    "2. Îã§ÏñëÌïú ÏïôÏÉÅÎ∏î Î∞©Î≤ï Ï†ÅÏö©\n",
    "3. ÏµúÏ†Å Ï°∞Ìï© Ï∞æÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d868922",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import timm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08198f26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5d7aa",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af60f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    BACKBONE_WEIGHTS = Path(\"/kaggle/input/pretrained-weights-biomass/dinov3_large/dinov3_large/dinov3_vitl16_qkvb.pth\")\n",
    "    \n",
    "    # Í∞Å Î≤ÑÏ†Ñ Î™®Îç∏ Í≤ΩÎ°ú (Kaggle DatasetÏúºÎ°ú ÏóÖÎ°úÎìú)\n",
    "    MODELS = {\n",
    "        'v20': Path(\"/kaggle/input/csiro-v20-models\"),\n",
    "        'v22': Path(\"/kaggle/input/csiro-v22-models\"),\n",
    "        'v23': Path(\"/kaggle/input/csiro-v23-models\"),\n",
    "        'v25': Path(\"/kaggle/input/csiro-v25-models\"),\n",
    "        'v26': Path(\"/kaggle/input/csiro-v26-models\"),\n",
    "    }\n",
    "    \n",
    "    model_name = \"vit_large_patch16_dinov3_qkvb.lvd1689m\"\n",
    "    img_size = (512, 512)\n",
    "    \n",
    "    hidden_dim = 512\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "    use_layernorm = True\n",
    "    veg_feat_dim = 128  # v25Ïö©\n",
    "    \n",
    "    batch_size = 16\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5d63c",
   "metadata": {},
   "source": [
    "## üìä Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce4bf9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, cfg, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(self.cfg.DATA_PATH / row['image_path']).convert('RGB')\n",
    "        width, height = img.size\n",
    "        mid = width // 2\n",
    "        \n",
    "        left_img = img.crop((0, 0, mid, height))\n",
    "        right_img = img.crop((mid, 0, width, height))\n",
    "        \n",
    "        if self.transform:\n",
    "            left_img = self.transform(left_img)\n",
    "            right_img = self.transform(right_img)\n",
    "        \n",
    "        return left_img, right_img, row['sample_id_prefix']\n",
    "\n",
    "\n",
    "def get_test_transform(cfg):\n",
    "    return T.Compose([\n",
    "        T.Resize(cfg.img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcdb88",
   "metadata": {},
   "source": [
    "## üß† Models (v20/v22/v23/v26 Ìò∏Ìôò)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b02abc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FiLM(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim // 2, feat_dim * 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, context):\n",
    "        return torch.chunk(self.mlp(context), 2, dim=1)\n",
    "\n",
    "\n",
    "def make_head(in_dim, hidden_dim, num_layers, dropout, use_layernorm):\n",
    "    layers = []\n",
    "    current_dim = in_dim\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "        if i < num_layers - 1:\n",
    "            if use_layernorm:\n",
    "                layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        current_dim = hidden_dim\n",
    "    layers.append(nn.Linear(hidden_dim, 1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class CSIROModelBase(nn.Module):\n",
    "    \"\"\"v20/v22/v23/v26 Ìò∏Ìôò Î™®Îç∏\"\"\"\n",
    "    def __init__(self, cfg, backbone_weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone_weights_path and Path(backbone_weights_path).exists():\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=False, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "            state = torch.load(backbone_weights_path, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(state, strict=False)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=True, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        combined_dim = feat_dim * 2\n",
    "        \n",
    "        self.film = FiLM(feat_dim)\n",
    "        \n",
    "        self.head_green = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers, \n",
    "                                    cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_clover = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers,\n",
    "                                     cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_dead = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers,\n",
    "                                   cfg.dropout, cfg.use_layernorm)\n",
    "        \n",
    "        self.head_height = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 1)\n",
    "        )\n",
    "        self.head_ndvi = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.backbone(left_img)\n",
    "        right_feat = self.backbone(right_img)\n",
    "        \n",
    "        context = (left_feat + right_feat) / 2\n",
    "        gamma, beta = self.film(context)\n",
    "        \n",
    "        left_mod = left_feat * (1 + gamma) + beta\n",
    "        right_mod = right_feat * (1 + gamma) + beta\n",
    "        \n",
    "        combined = torch.cat([left_mod, right_mod], dim=1)\n",
    "        \n",
    "        green = self.softplus(self.head_green(combined))\n",
    "        clover = self.softplus(self.head_clover(combined))\n",
    "        dead = self.softplus(self.head_dead(combined))\n",
    "        \n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        \n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "\n",
    "\n",
    "class CSIROModelV22(nn.Module):\n",
    "    \"\"\"v22 Ï†ÑÏö© Î™®Îç∏ (Frozen backbone, Îçî ÏûëÏùÄ head)\"\"\"\n",
    "    def __init__(self, cfg, backbone_weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone_weights_path and Path(backbone_weights_path).exists():\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=False, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "            state = torch.load(backbone_weights_path, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(state, strict=False)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=True, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        combined_dim = feat_dim * 2\n",
    "        \n",
    "        self.film = FiLM(feat_dim)\n",
    "        \n",
    "        # v22: hidden_dim=256, num_layers=2\n",
    "        v22_hidden = 256\n",
    "        v22_layers = 2\n",
    "        v22_dropout = 0.3\n",
    "        \n",
    "        self.head_green = self._make_head_v22(combined_dim, v22_hidden, v22_layers, v22_dropout)\n",
    "        self.head_clover = self._make_head_v22(combined_dim, v22_hidden, v22_layers, v22_dropout)\n",
    "        self.head_dead = self._make_head_v22(combined_dim, v22_hidden, v22_layers, v22_dropout)\n",
    "        \n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "    \n",
    "    def _make_head_v22(self, in_dim, hidden_dim, num_layers, dropout):\n",
    "        layers = []\n",
    "        current_dim = in_dim\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if i < num_layers - 1:\n",
    "                layers.append(nn.LayerNorm(hidden_dim))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            current_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.backbone(left_img)\n",
    "        right_feat = self.backbone(right_img)\n",
    "        \n",
    "        context = (left_feat + right_feat) / 2\n",
    "        gamma, beta = self.film(context)\n",
    "        \n",
    "        left_mod = left_feat * (1 + gamma) + beta\n",
    "        right_mod = right_feat * (1 + gamma) + beta\n",
    "        \n",
    "        combined = torch.cat([left_mod, right_mod], dim=1)\n",
    "        \n",
    "        green = self.softplus(self.head_green(combined))\n",
    "        clover = self.softplus(self.head_clover(combined))\n",
    "        dead = self.softplus(self.head_dead(combined))\n",
    "        \n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        \n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "\n",
    "\n",
    "class VegetationEncoder(nn.Module):\n",
    "    \"\"\"v25Ïö© Vegetation Index Encoder\"\"\"\n",
    "    def __init__(self, in_channels=2, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(), nn.Linear(128, out_dim), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class CSIROModelV25(nn.Module):\n",
    "    \"\"\"v25 Ï†ÑÏö© Î™®Îç∏ (VegIdx Fusion)\"\"\"\n",
    "    def __init__(self, cfg, backbone_weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone_weights_path and Path(backbone_weights_path).exists():\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=False, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "            state = torch.load(backbone_weights_path, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(state, strict=False)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(cfg.model_name, pretrained=True, \n",
    "                                               num_classes=0, global_pool='avg')\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.veg_encoder = VegetationEncoder(in_channels=2, out_dim=cfg.veg_feat_dim)\n",
    "        self.film = FiLM(feat_dim)\n",
    "        \n",
    "        combined_dim = feat_dim * 2 + cfg.veg_feat_dim * 2\n",
    "        \n",
    "        self.head_green = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers, \n",
    "                                    cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_clover = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers,\n",
    "                                     cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_dead = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers,\n",
    "                                   cfg.dropout, cfg.use_layernorm)\n",
    "        \n",
    "        self.head_height = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 1)\n",
    "        )\n",
    "        self.head_ndvi = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "    \n",
    "    def forward(self, left_rgb, right_rgb, left_veg, right_veg):\n",
    "        left_feat = self.backbone(left_rgb)\n",
    "        right_feat = self.backbone(right_rgb)\n",
    "        \n",
    "        left_veg_feat = self.veg_encoder(left_veg)\n",
    "        right_veg_feat = self.veg_encoder(right_veg)\n",
    "        \n",
    "        context = (left_feat + right_feat) / 2\n",
    "        gamma, beta = self.film(context)\n",
    "        \n",
    "        left_mod = left_feat * (1 + gamma) + beta\n",
    "        right_mod = right_feat * (1 + gamma) + beta\n",
    "        \n",
    "        combined = torch.cat([left_mod, right_mod, left_veg_feat, right_veg_feat], dim=1)\n",
    "        \n",
    "        green = self.softplus(self.head_green(combined))\n",
    "        clover = self.softplus(self.head_clover(combined))\n",
    "        dead = self.softplus(self.head_dead(combined))\n",
    "        \n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        \n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44355b80",
   "metadata": {},
   "source": [
    "## üîÆ Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986a16d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_base_model(model, loader, device):\n",
    "    \"\"\"v20/v22/v23/v26 ÏòàÏ∏°\"\"\"\n",
    "    model.eval()\n",
    "    all_outputs, all_ids = [], []\n",
    "    \n",
    "    for left, right, ids in tqdm(loader, desc=\"Predicting\"):\n",
    "        left = left.to(device)\n",
    "        right = right.to(device)\n",
    "        outputs = model(left, right)\n",
    "        all_outputs.append(outputs.cpu().numpy())\n",
    "        all_ids.extend(ids)\n",
    "    \n",
    "    return np.concatenate(all_outputs), all_ids\n",
    "\n",
    "\n",
    "def compute_vegetation_indices(img_array):\n",
    "    \"\"\"RGBÏóêÏÑú Vegetation Index Í≥ÑÏÇ∞\"\"\"\n",
    "    img = img_array.astype(np.float32) / 255.0\n",
    "    r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "    exg = (2*g - r - b + 2) / 4\n",
    "    gr_ratio = np.clip(g / (r + 1e-8), 0, 3) / 3\n",
    "    return np.stack([exg, gr_ratio], axis=-1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_v25_model(model, test_df, cfg, device):\n",
    "    \"\"\"v25 Ï†ÑÏö© ÏòàÏ∏° (VegIdx Ìè¨Ìï®)\"\"\"\n",
    "    model.eval()\n",
    "    all_outputs, all_ids = [], []\n",
    "    \n",
    "    transform = get_test_transform(cfg)\n",
    "    \n",
    "    for idx in tqdm(range(len(test_df)), desc=\"Predicting v25\"):\n",
    "        row = test_df.iloc[idx]\n",
    "        img = Image.open(cfg.DATA_PATH / row['image_path']).convert('RGB')\n",
    "        width, height = img.size\n",
    "        mid = width // 2\n",
    "        \n",
    "        left_pil = img.crop((0, 0, mid, height)).resize(cfg.img_size)\n",
    "        right_pil = img.crop((mid, 0, width, height)).resize(cfg.img_size)\n",
    "        \n",
    "        # RGB\n",
    "        left_rgb = transform(left_pil).unsqueeze(0).to(device)\n",
    "        right_rgb = transform(right_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # VegIdx\n",
    "        left_np = np.array(left_pil)\n",
    "        right_np = np.array(right_pil)\n",
    "        left_veg = torch.from_numpy(compute_vegetation_indices(left_np)).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "        right_veg = torch.from_numpy(compute_vegetation_indices(right_np)).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "        \n",
    "        outputs = model(left_rgb, right_rgb, left_veg, right_veg)\n",
    "        all_outputs.append(outputs.cpu().numpy())\n",
    "        all_ids.append(row['sample_id_prefix'])\n",
    "    \n",
    "    return np.concatenate(all_outputs), all_ids\n",
    "\n",
    "\n",
    "def predict_version(version, model_dir, test_df, cfg, device):\n",
    "    \"\"\"Î≤ÑÏ†ÑÎ≥Ñ ÏïôÏÉÅÎ∏î ÏòàÏ∏°\"\"\"\n",
    "    model_files = sorted(model_dir.glob(\"model_fold*.pth\"))\n",
    "    if not model_files:\n",
    "        print(f\"‚ö†Ô∏è No models found for {version}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n=== {version}: {len(model_files)} models ===\")\n",
    "    \n",
    "    all_fold_preds = []\n",
    "    final_ids = None\n",
    "    \n",
    "    if version == 'v25':\n",
    "        # v25Îäî Î≥ÑÎèÑ Ï≤òÎ¶¨ (VegIdx Ìè¨Ìï®)\n",
    "        for model_file in model_files:\n",
    "            print(f\"Loading {model_file.name}...\")\n",
    "            model = CSIROModelV25(cfg, cfg.BACKBONE_WEIGHTS).to(device)\n",
    "            model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "            \n",
    "            preds, ids = predict_v25_model(model, test_df, cfg, device)\n",
    "            all_fold_preds.append(preds)\n",
    "            if final_ids is None:\n",
    "                final_ids = ids\n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    elif version == 'v22':\n",
    "        # v22Îäî Î≥ÑÎèÑ Î™®Îç∏ Íµ¨Ï°∞ (hidden_dim=256, num_layers=2)\n",
    "        transform = get_test_transform(cfg)\n",
    "        dataset = TestDataset(test_df, cfg, transform)\n",
    "        loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                           num_workers=cfg.num_workers, pin_memory=True)\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            print(f\"Loading {model_file.name}...\")\n",
    "            model = CSIROModelV22(cfg, cfg.BACKBONE_WEIGHTS).to(device)\n",
    "            model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "            \n",
    "            preds, ids = predict_base_model(model, loader, device)\n",
    "            all_fold_preds.append(preds)\n",
    "            if final_ids is None:\n",
    "                final_ids = ids\n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        # v20/v23/v26\n",
    "        transform = get_test_transform(cfg)\n",
    "        dataset = TestDataset(test_df, cfg, transform)\n",
    "        loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                           num_workers=cfg.num_workers, pin_memory=True)\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            print(f\"Loading {model_file.name}...\")\n",
    "            model = CSIROModelBase(cfg, cfg.BACKBONE_WEIGHTS).to(device)\n",
    "            model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "            \n",
    "            preds, ids = predict_base_model(model, loader, device)\n",
    "            all_fold_preds.append(preds)\n",
    "            if final_ids is None:\n",
    "                final_ids = ids\n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.mean(all_fold_preds, axis=0), final_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48d1bd",
   "metadata": {},
   "source": [
    "## üîß Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62524ebf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def simple_average(predictions_dict):\n",
    "    \"\"\"Îã®Ïàú ÌèâÍ∑†\"\"\"\n",
    "    preds = [v for v in predictions_dict.values() if v is not None]\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "def weighted_average(predictions_dict, weights):\n",
    "    \"\"\"Í∞ÄÏ§ë ÌèâÍ∑†\"\"\"\n",
    "    result = np.zeros_like(list(predictions_dict.values())[0])\n",
    "    total_weight = 0\n",
    "    \n",
    "    for version, pred in predictions_dict.items():\n",
    "        if pred is not None and version in weights:\n",
    "            result += weights[version] * pred\n",
    "            total_weight += weights[version]\n",
    "    \n",
    "    return result / total_weight\n",
    "\n",
    "\n",
    "def rank_average(predictions_dict):\n",
    "    \"\"\"ÏàúÏúÑ Í∏∞Î∞ò ÌèâÍ∑† (outlierÏóê Í∞ïÌï®) - Ï†ïÍ∑úÌôîÎêú ÏàúÏúÑ ÌèâÍ∑†\"\"\"\n",
    "    preds_list = [v for v in predictions_dict.values() if v is not None]\n",
    "    n_models = len(preds_list)\n",
    "    n_samples, n_targets = preds_list[0].shape\n",
    "    \n",
    "    # Í∞Å Î™®Îç∏Î≥Ñ, ÌÉÄÍ≤üÎ≥ÑÎ°ú ÏàúÏúÑ Í≥ÑÏÇ∞ (0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî)\n",
    "    normalized_ranks = []\n",
    "    for pred in preds_list:\n",
    "        rank = np.zeros_like(pred)\n",
    "        for t in range(n_targets):\n",
    "            r = rankdata(pred[:, t])\n",
    "            rank[:, t] = (r - 1) / (n_samples - 1)  # 0-1 Î≤îÏúÑ\n",
    "        normalized_ranks.append(rank)\n",
    "    \n",
    "    # Ï†ïÍ∑úÌôîÎêú ÏàúÏúÑ ÌèâÍ∑†\n",
    "    avg_rank = np.mean(normalized_ranks, axis=0)\n",
    "    \n",
    "    # Í∞Å ÌÉÄÍ≤üÎ≥ÑÎ°ú Ï†ïÎ†¨Îêú Í∞íÏóêÏÑú Î≥¥Í∞Ñ\n",
    "    result = np.zeros((n_samples, n_targets))\n",
    "    \n",
    "    for t in range(n_targets):\n",
    "        # Î™®Îì† Î™®Îç∏Ïùò Ìï¥Îãπ ÌÉÄÍ≤ü Í∞í Í≤∞Ìï©\n",
    "        all_vals = np.concatenate([p[:, t] for p in preds_list])\n",
    "        sorted_vals = np.sort(all_vals)\n",
    "        \n",
    "        # ÌèâÍ∑† ÏàúÏúÑÏóê Ìï¥ÎãπÌïòÎäî Í∞í Î≥¥Í∞Ñ\n",
    "        for i in range(n_samples):\n",
    "            idx = avg_rank[i, t] * (len(sorted_vals) - 1)\n",
    "            idx_low = int(np.floor(idx))\n",
    "            idx_high = min(idx_low + 1, len(sorted_vals) - 1)\n",
    "            frac = idx - idx_low\n",
    "            result[i, t] = sorted_vals[idx_low] * (1 - frac) + sorted_vals[idx_high] * frac\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def target_wise_best(predictions_dict, oof_scores=None):\n",
    "    \"\"\"ÌÉÄÍ≤üÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ ÏÑ†ÌÉù (OOF Í∏∞Î∞ò)\"\"\"\n",
    "    if oof_scores is None:\n",
    "        # OOF ÏóÜÏúºÎ©¥ Îã®Ïàú ÌèâÍ∑†\n",
    "        return simple_average(predictions_dict)\n",
    "    \n",
    "    versions = [v for v in predictions_dict.keys() if predictions_dict[v] is not None]\n",
    "    n_samples, n_targets = list(predictions_dict.values())[0].shape\n",
    "    result = np.zeros((n_samples, n_targets))\n",
    "    \n",
    "    for t_idx, target in enumerate(TARGET_ORDER):\n",
    "        # Ìï¥Îãπ ÌÉÄÍ≤üÏóêÏÑú Í∞ÄÏû• Ï¢ãÏùÄ Î™®Îç∏ ÏÑ†ÌÉù\n",
    "        best_version = max(versions, key=lambda v: oof_scores.get(v, {}).get(target, 0))\n",
    "        result[:, t_idx] = predictions_dict[best_version][:, t_idx]\n",
    "        print(f\"  {target}: {best_version}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418bbcd",
   "metadata": {},
   "source": [
    "## üìã Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_df['sample_id_prefix'] = test_df['sample_id'].str.split('__').str[0]\n",
    "test_wide = test_df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "print(f\"Test samples: {len(test_wide)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f583ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ v27: Breakthrough Multi-Model Inference\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Í∞Å Î≤ÑÏ†Ñ ÏòàÏ∏° ÏàòÏßë\n",
    "predictions = {}\n",
    "sample_ids = None\n",
    "\n",
    "for version, model_dir in cfg.MODELS.items():\n",
    "    if model_dir.exists():\n",
    "        preds, ids = predict_version(version, model_dir, test_wide, cfg, cfg.device)\n",
    "        if preds is not None:\n",
    "            predictions[version] = preds\n",
    "            if sample_ids is None:\n",
    "                sample_ids = ids\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {version} not found: {model_dir}\")\n",
    "\n",
    "print(f\"\\n‚úì Collected predictions from: {list(predictions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daabbd",
   "metadata": {},
   "source": [
    "## üéØ Ensemble & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d526a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Ïó¨Í∏∞ÏÑú ÏïôÏÉÅÎ∏î Î∞©Î≤ï ÏÑ†ÌÉù!\n",
    "# ÏòµÏÖò: \"simple\", \"rank\", \"weighted\"\n",
    "ENSEMBLE_METHOD = \"simple\"\n",
    "\n",
    "# Í∞ÄÏ§ë ÌèâÍ∑†Ïö© Í∞ÄÏ§ëÏπò (ENSEMBLE_METHOD = \"weighted\" ÏÑ†ÌÉù Ïãú ÏÇ¨Ïö©)\n",
    "WEIGHTS = {'v20': 1.0, 'v22': 0.8, 'v23': 1.0, 'v25': 0.9, 'v26': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9990b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Generating Ensemble ===\")\n",
    "\n",
    "if ENSEMBLE_METHOD == \"simple\":\n",
    "    final_preds = simple_average(predictions)\n",
    "    print(\"‚úì Method: Simple Average\")\n",
    "elif ENSEMBLE_METHOD == \"rank\":\n",
    "    final_preds = rank_average(predictions)\n",
    "    print(\"‚úì Method: Rank Average\")\n",
    "elif ENSEMBLE_METHOD == \"weighted\":\n",
    "    final_preds = weighted_average(predictions, WEIGHTS)\n",
    "    print(f\"‚úì Method: Weighted Average\")\n",
    "    print(f\"  Weights: {WEIGHTS}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown method: {ENSEMBLE_METHOD}\")\n",
    "\n",
    "print(f\"  Shape: {final_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏòàÏ∏° ÌÜµÍ≥Ñ\n",
    "print(\"\\n=== Prediction Statistics ===\")\n",
    "print(f\"{'Target':<15} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "for idx, target in enumerate(TARGET_ORDER):\n",
    "    vals = final_preds[:, idx]\n",
    "    print(f\"{target:<15} {vals.mean():>10.2f} {vals.std():>10.2f} {vals.min():>10.2f} {vals.max():>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6d1ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Submission ÏÉùÏÑ±\n",
    "pred_df = pd.DataFrame(final_preds, columns=TARGET_ORDER)\n",
    "pred_df['sample_id_prefix'] = sample_ids\n",
    "\n",
    "sub_df = pred_df.melt(\n",
    "    id_vars=['sample_id_prefix'],\n",
    "    value_vars=TARGET_ORDER,\n",
    "    var_name='target_name',\n",
    "    value_name='target'\n",
    ")\n",
    "sub_df['sample_id'] = sub_df['sample_id_prefix'] + '__' + sub_df['target_name']\n",
    "\n",
    "submission = sub_df[['sample_id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Í≤ÄÏ¶ù\n",
    "sample_sub = pd.read_csv(cfg.DATA_PATH / \"sample_submission.csv\")\n",
    "assert len(submission) == len(sample_sub), \"Format mismatch!\"\n",
    "\n",
    "print(f\"\\n‚úÖ submission.csv saved ({ENSEMBLE_METHOD} method)\")\n",
    "print(f\"   {len(submission)} rows\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
