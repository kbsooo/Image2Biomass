{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10af0932",
   "metadata": {},
   "source": [
    "# üéØ v17b: CV-LB Gap Reduction Inference\n",
    "\n",
    "**Î™©Ìëú**: CV-LB Gap ÏµúÏÜåÌôî (0.10 ‚Üí 0.05 Ïù¥Ìïò)\n",
    "\n",
    "**Ï†ÑÎûµ**:\n",
    "1. 7x TTA (Original + Flips + Rotations)\n",
    "2. ÏòàÏ∏°Í∞í Clipping (outlier Ï†úÍ±∞)\n",
    "3. Ensemble ÌèâÍ∑† ÎåÄÏã† Median ÏÇ¨Ïö© ÏòµÏÖò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fd1d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import timm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e82dc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fc51b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf34be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    \n",
    "    # ‚ö†Ô∏è Ïù¥ Í≤ΩÎ°úÎ•º ÏóÖÎ°úÎìúÌïú Î™®Îç∏ Dataset Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî\n",
    "    MODELS_DIR = Path(\"/kaggle/input/csiro-v17-models\")\n",
    "    \n",
    "    # ÌëúÏ§Ä ViT-Large ÏïÑÌÇ§ÌÖçÏ≤ò (timmÏù¥ Ïù∏Ïãù Í∞ÄÎä•)\n",
    "    # Ïã§Ï†ú weightsÎäî fold modelÏóêÏÑú Î°úÎìúÎê®\n",
    "    model_name = \"vit_large_patch16_224\"\n",
    "    img_size = (512, 512)\n",
    "    \n",
    "    # v17Í≥º ÎèôÏùºÌïú head Íµ¨Ï°∞\n",
    "    hidden_dim = 512\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "    use_layernorm = True\n",
    "    \n",
    "    # === CV-LB Gap Í∞êÏÜå Ï†ÑÎûµ ===\n",
    "    use_strong_tta = True      # 7x TTA\n",
    "    use_clipping = True        # Prediction clipping\n",
    "    use_median = False         # Median ensemble (alternative)\n",
    "    \n",
    "    # Clipping bounds (train Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)\n",
    "    clip_bounds = {\n",
    "        'Dry_Green_g': (0, 150),\n",
    "        'Dry_Dead_g': (0, 150),\n",
    "        'Dry_Clover_g': (0, 80),\n",
    "        'GDM_g': (0, 200),\n",
    "        'Dry_Total_g': (0, 250),\n",
    "    }\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = CFG()\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Strong TTA: {cfg.use_strong_tta}\")\n",
    "print(f\"Clipping: {cfg.use_clipping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244f074",
   "metadata": {},
   "source": [
    "## üìä Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7e467",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, cfg, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(self.cfg.DATA_PATH / row['image_path']).convert('RGB')\n",
    "        width, height = img.size\n",
    "        mid = width // 2\n",
    "        \n",
    "        left_img = img.crop((0, 0, mid, height))\n",
    "        right_img = img.crop((mid, 0, width, height))\n",
    "        \n",
    "        if self.transform:\n",
    "            left_img = self.transform(left_img)\n",
    "            right_img = self.transform(right_img)\n",
    "        \n",
    "        return left_img, right_img, row['sample_id_prefix']\n",
    "\n",
    "def get_tta_dataloaders(df, cfg):\n",
    "    \"\"\"7x TTA: Original + Flips + Rotations\"\"\"\n",
    "    loaders = []\n",
    "    \n",
    "    base_transform = [\n",
    "        T.Resize(cfg.img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    "    \n",
    "    if cfg.use_strong_tta:\n",
    "        # 7x TTA\n",
    "        transforms_list = [\n",
    "            # 1. Original\n",
    "            T.Compose(base_transform),\n",
    "            # 2. Horizontal Flip\n",
    "            T.Compose([T.Resize(cfg.img_size), T.RandomHorizontalFlip(p=1.0)] + base_transform[1:]),\n",
    "            # 3. Vertical Flip\n",
    "            T.Compose([T.Resize(cfg.img_size), T.RandomVerticalFlip(p=1.0)] + base_transform[1:]),\n",
    "            # 4. Rotate 90\n",
    "            T.Compose([T.Resize(cfg.img_size), T.Lambda(lambda x: TF.rotate(x, 90))] + base_transform[1:]),\n",
    "            # 5. Rotate 180\n",
    "            T.Compose([T.Resize(cfg.img_size), T.Lambda(lambda x: TF.rotate(x, 180))] + base_transform[1:]),\n",
    "            # 6. Rotate 270\n",
    "            T.Compose([T.Resize(cfg.img_size), T.Lambda(lambda x: TF.rotate(x, 270))] + base_transform[1:]),\n",
    "            # 7. HFlip + Rotate 90\n",
    "            T.Compose([T.Resize(cfg.img_size), T.RandomHorizontalFlip(p=1.0), \n",
    "                      T.Lambda(lambda x: TF.rotate(x, 90))] + base_transform[1:]),\n",
    "        ]\n",
    "        print(f\"Using 7x TTA\")\n",
    "    else:\n",
    "        # 3x TTA\n",
    "        transforms_list = [\n",
    "            T.Compose(base_transform),\n",
    "            T.Compose([T.Resize(cfg.img_size), T.RandomHorizontalFlip(p=1.0)] + base_transform[1:]),\n",
    "            T.Compose([T.Resize(cfg.img_size), T.RandomVerticalFlip(p=1.0)] + base_transform[1:]),\n",
    "        ]\n",
    "        print(f\"Using 3x TTA\")\n",
    "    \n",
    "    for transform in transforms_list:\n",
    "        dataset = TestDataset(df, cfg, transform)\n",
    "        loader = DataLoader(dataset, batch_size=cfg.batch_size,\n",
    "                           shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1cda2",
   "metadata": {},
   "source": [
    "## üß† Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6236c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FiLM(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, feat_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feat_dim // 2, feat_dim * 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, context):\n",
    "        gamma_beta = self.mlp(context)\n",
    "        return torch.chunk(gamma_beta, 2, dim=1)\n",
    "\n",
    "\n",
    "def make_head(in_dim: int, hidden_dim: int, num_layers: int, dropout: float, use_layernorm: bool):\n",
    "    layers = []\n",
    "    current_dim = in_dim\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        out_dim = hidden_dim if i < num_layers - 1 else 1\n",
    "        layers.append(nn.Linear(current_dim, out_dim if i < num_layers - 1 else hidden_dim))\n",
    "        \n",
    "        if i < num_layers - 1:\n",
    "            if use_layernorm:\n",
    "                layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        current_dim = hidden_dim\n",
    "    \n",
    "    layers.append(nn.Linear(hidden_dim, 1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class CSIROModelV17(nn.Module):\n",
    "    \"\"\"v17 Î™®Îç∏ - ÌëúÏ§Ä ViT ÏïÑÌÇ§ÌÖçÏ≤òÎ°ú ÏãúÏûë, fold weightsÎ°ú ÎçÆÏñ¥ÏîÄ\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ÌëúÏ§Ä ViT-Large ÏïÑÌÇ§ÌÖçÏ≤ò (timmÏù¥ Ïù∏Ïãù Í∞ÄÎä•)\n",
    "        # pretrained=FalseÎ°ú Îπà Î™®Îç∏ ÏÉùÏÑ±, ÎÇòÏ§ëÏóê fold weightsÎ°ú Ï±ÑÏõÄ\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=False, num_classes=0, global_pool='avg')\n",
    "        \n",
    "        feat_dim = self.backbone.num_features  # 1024\n",
    "        combined_dim = feat_dim * 2\n",
    "        \n",
    "        self.film = FiLM(feat_dim)\n",
    "        \n",
    "        self.head_green = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers, cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_clover = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers, cfg.dropout, cfg.use_layernorm)\n",
    "        self.head_dead = make_head(combined_dim, cfg.hidden_dim, cfg.num_layers, cfg.dropout, cfg.use_layernorm)\n",
    "        \n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "    \n",
    "    def forward(self, left_img, right_img):\n",
    "        left_feat = self.backbone(left_img)\n",
    "        right_feat = self.backbone(right_img)\n",
    "        \n",
    "        context = (left_feat + right_feat) / 2\n",
    "        gamma, beta = self.film(context)\n",
    "        \n",
    "        left_mod = left_feat * (1 + gamma) + beta\n",
    "        right_mod = right_feat * (1 + gamma) + beta\n",
    "        \n",
    "        combined = torch.cat([left_mod, right_mod], dim=1)\n",
    "        \n",
    "        green = self.softplus(self.head_green(combined))\n",
    "        clover = self.softplus(self.head_clover(combined))\n",
    "        dead = self.softplus(self.head_dead(combined))\n",
    "        \n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        \n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce5025",
   "metadata": {},
   "source": [
    "## üîÆ Inference with Gap Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671dd9e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    all_outputs, all_ids = [], []\n",
    "    \n",
    "    for left, right, ids in tqdm(loader, desc=\"Predicting\"):\n",
    "        left = left.to(device)\n",
    "        right = right.to(device)\n",
    "        outputs = model(left, right)\n",
    "        all_outputs.append(outputs.cpu().numpy())\n",
    "        all_ids.extend(ids)\n",
    "    \n",
    "    return np.concatenate(all_outputs), all_ids\n",
    "\n",
    "\n",
    "def predict_with_tta(model, tta_loaders, device, use_median=False):\n",
    "    all_preds = []\n",
    "    final_ids = None\n",
    "    \n",
    "    for loader in tta_loaders:\n",
    "        preds, ids = predict(model, loader, device)\n",
    "        all_preds.append(preds)\n",
    "        if final_ids is None:\n",
    "            final_ids = ids\n",
    "    \n",
    "    if use_median:\n",
    "        # MedianÏùÄ outlierÏóê Îçî robust\n",
    "        avg_preds = np.median(all_preds, axis=0)\n",
    "    else:\n",
    "        avg_preds = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    return avg_preds, final_ids\n",
    "\n",
    "\n",
    "def clip_predictions(preds, cfg):\n",
    "    \"\"\"Outlier ÏòàÏ∏°Í∞í clipping\"\"\"\n",
    "    TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    \n",
    "    clipped = preds.copy()\n",
    "    for i, target in enumerate(TARGET_ORDER):\n",
    "        low, high = cfg.clip_bounds[target]\n",
    "        original_min, original_max = clipped[:, i].min(), clipped[:, i].max()\n",
    "        clipped[:, i] = np.clip(clipped[:, i], low, high)\n",
    "        new_min, new_max = clipped[:, i].min(), clipped[:, i].max()\n",
    "        if original_max > high or original_min < low:\n",
    "            print(f\"  {target}: [{original_min:.1f}, {original_max:.1f}] ‚Üí [{new_min:.1f}, {new_max:.1f}]\")\n",
    "    \n",
    "    return clipped\n",
    "\n",
    "\n",
    "def predict_ensemble(cfg, tta_loaders):\n",
    "    all_fold_preds = []\n",
    "    final_ids = None\n",
    "    \n",
    "    model_files = sorted(cfg.MODELS_DIR.glob(\"model_fold*.pth\"))\n",
    "    print(f\"Found {len(model_files)} models\")\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        print(f\"\\nLoading {model_file.name}...\")\n",
    "        \n",
    "        # 1. Îπà Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏÉùÏÑ±\n",
    "        model = CSIROModelV17(cfg).to(cfg.device)\n",
    "        \n",
    "        # 2. fold weights Î°úÎìú (backbone Ìè¨Ìï® Ï†ÑÏ≤¥ ÎçÆÏñ¥ÏîÄ)\n",
    "        state_dict = torch.load(model_file, map_location=cfg.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"‚úì Weights loaded\")\n",
    "        \n",
    "        preds, ids = predict_with_tta(model, tta_loaders, cfg.device, cfg.use_median)\n",
    "        all_fold_preds.append(preds)\n",
    "        \n",
    "        if final_ids is None:\n",
    "            final_ids = ids\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Fold ensemble\n",
    "    if cfg.use_median:\n",
    "        final_preds = np.median(all_fold_preds, axis=0)\n",
    "    else:\n",
    "        final_preds = np.mean(all_fold_preds, axis=0)\n",
    "    \n",
    "    # Clipping\n",
    "    if cfg.use_clipping:\n",
    "        print(\"\\nüìè Applying prediction clipping...\")\n",
    "        final_preds = clip_predictions(final_preds, cfg)\n",
    "    \n",
    "    return final_preds, final_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57652502",
   "metadata": {},
   "source": [
    "## üìã Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54514f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_df['sample_id_prefix'] = test_df['sample_id'].str.split('__').str[0]\n",
    "test_wide = test_df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "print(f\"Test samples: {len(test_wide)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_loaders = get_tta_dataloaders(test_wide, cfg)\n",
    "preds, sample_ids = predict_ensemble(cfg, tta_loaders)\n",
    "print(f\"\\nPredictions: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78917e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "pred_df = pd.DataFrame(preds, columns=TARGET_ORDER)\n",
    "pred_df['sample_id_prefix'] = sample_ids\n",
    "\n",
    "sub_df = pred_df.melt(\n",
    "    id_vars=['sample_id_prefix'],\n",
    "    value_vars=TARGET_ORDER,\n",
    "    var_name='target_name',\n",
    "    value_name='target'\n",
    ")\n",
    "sub_df['sample_id'] = sub_df['sample_id_prefix'] + '__' + sub_df['target_name']\n",
    "\n",
    "submission = sub_df[['sample_id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {len(submission)} rows\")\n",
    "print(\"\\nPrediction statistics:\")\n",
    "for col in TARGET_ORDER:\n",
    "    vals = pred_df[col]\n",
    "    print(f\"  {col}: mean={vals.mean():.2f}, std={vals.std():.2f}, min={vals.min():.2f}, max={vals.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf46017",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(cfg.DATA_PATH / \"sample_submission.csv\")\n",
    "assert len(submission) == len(sample_sub), \"Row count mismatch!\"\n",
    "print(\"\\n‚úì Format verified!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
