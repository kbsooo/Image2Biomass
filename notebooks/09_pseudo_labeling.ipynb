{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e216867d",
   "metadata": {},
   "source": [
    "# üî¨ Exp B: Pseudo-Labeling\n",
    "\n",
    "**ÏïÑÏù¥ÎîîÏñ¥**: \n",
    "1. Image ‚Üí NDVI/Height ÏòàÏ∏° Î™®Îç∏ ÌïôÏäµ (Train Îç∞Ïù¥ÌÑ∞Î°ú)\n",
    "2. Test Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ NDVI/Height ÏòàÏ∏°\n",
    "3. ÏòàÏ∏°Îêú pseudo tabularÎ°ú Multi-Modal ÌïôÏäµ\n",
    "\n",
    "**Ïû•Ï†ê**: TestÏóêÏÑúÎèÑ tabular Ï†ïÎ≥¥ ÌôúÏö© Í∞ÄÎä•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b5ff9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185445a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdea895",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17755b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "    WEIGHTS_PATH = Path(\"/kaggle/input/pretrained-weights-biomass\")\n",
    "    \n",
    "    backbone = \"efficientnet_b4\"\n",
    "    input_size = 384\n",
    "    \n",
    "    # Phase 1: Tabular ÏòàÏ∏° Î™®Îç∏\n",
    "    tabular_epochs = 10\n",
    "    tabular_lr = 2e-4\n",
    "    \n",
    "    # Phase 2: Biomass ÏòàÏ∏° Î™®Îç∏\n",
    "    n_folds = 5\n",
    "    train_folds = 1  # ÏãπÏàò ÌôïÏù∏\n",
    "    epochs = 10\n",
    "    batch_size = 16\n",
    "    lr = 2e-4\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    tabular_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "    \n",
    "    seed = 42\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    independent_targets = ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']\n",
    "    all_targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "cfg = CFG()\n",
    "cfg.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fafd6",
   "metadata": {},
   "source": [
    "## Phase 1: Image ‚Üí Tabular ÏòàÏ∏° Î™®Îç∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243215d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TabularPredictorDataset(Dataset):\n",
    "    \"\"\"Image ‚Üí NDVI/Height ÏòàÏ∏°ÏùÑ ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n",
    "    def __init__(self, df, cfg, transforms=None, scaler=None, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        tabular_data = df[cfg.tabular_cols].values.astype(np.float32)\n",
    "        if scaler is not None:\n",
    "            if mode == 'train':\n",
    "                self.targets = scaler.fit_transform(tabular_data)\n",
    "            else:\n",
    "                self.targets = scaler.transform(tabular_data)\n",
    "        else:\n",
    "            self.targets = tabular_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        targets = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4134ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TabularPredictor(nn.Module):\n",
    "    \"\"\"Image ‚Üí NDVI/Height ÏòàÏ∏° Î™®Îç∏\"\"\"\n",
    "    def __init__(self, backbone_name, n_outputs=2, pretrained=True, weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained and weights_path and Path(weights_path).exists():\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "            weights = torch.load(weights_path, weights_only=True)\n",
    "            weights = {k: v for k, v in weights.items() if not k.startswith('classifier')}\n",
    "            self.backbone.load_state_dict(weights, strict=False)\n",
    "            print(f\"‚úì Loaded pretrained weights\")\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb931a6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode: str = 'train', size: int = 384) -> A.Compose:\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.3, hue=0.05, p=0.7),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ab0f9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, is_train=True):\n",
    "    if 'target' in df.columns:\n",
    "        df_wide = pd.pivot_table(\n",
    "            df, values='target',\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name', aggfunc='mean'\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        df['target'] = 0\n",
    "        cols = ['image_path']\n",
    "        for col in ['Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']:\n",
    "            if col in df.columns:\n",
    "                cols.append(col)\n",
    "        df_wide = df.drop_duplicates(subset=['image_path'])[cols].reset_index(drop=True)\n",
    "    return df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e237a02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(cfg.DATA_PATH / \"train.csv\")\n",
    "train_wide = prepare_data(train_df, is_train=True)\n",
    "\n",
    "# KFold\n",
    "kf = KFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "train_wide['fold'] = -1\n",
    "for fold, (_, val_idx) in enumerate(kf.split(train_wide)):\n",
    "    train_wide.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Train data: {len(train_wide)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c30f0",
   "metadata": {},
   "source": [
    "## Phase 1: Train Tabular Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3dcad4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_tabular_predictor(train_df, cfg):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä Phase 1: Training Tabular Predictor (Image ‚Üí NDVI/Height)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Use all data for tabular predictor (or split if needed)\n",
    "    train_data = train_df[train_df['fold'] != 0].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_dataset = TabularPredictorDataset(\n",
    "        train_data, cfg, get_transforms('train', cfg.input_size), scaler, 'train'\n",
    "    )\n",
    "    val_dataset = TabularPredictorDataset(\n",
    "        val_data, cfg, get_transforms('val', cfg.input_size), scaler, 'val'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=cfg.num_workers)\n",
    "    \n",
    "    weights_path = None\n",
    "    if cfg.WEIGHTS_PATH.exists():\n",
    "        weights_path = str(cfg.WEIGHTS_PATH / cfg.backbone / f\"{cfg.backbone}.pth\")\n",
    "    \n",
    "    model = TabularPredictor(cfg.backbone, n_outputs=2, pretrained=True, weights_path=weights_path)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(cfg.device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.tabular_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.tabular_epochs)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.tabular_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, targets in tqdm(train_loader, desc='Train', leave=False):\n",
    "            imgs, targets = imgs.to(cfg.device), targets.to(cfg.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = F.mse_loss(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs, targets = imgs.to(cfg.device), targets.to(cfg.device)\n",
    "                preds = model(imgs)\n",
    "                loss = F.mse_loss(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{cfg.tabular_epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'scaler': scaler,\n",
    "            }, cfg.OUTPUT_DIR / 'tabular_predictor.pt')\n",
    "            print(f\"  ‚úì Saved!\")\n",
    "    \n",
    "    flush()\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfe1d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tabular_scaler = train_tabular_predictor(train_wide, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d8157",
   "metadata": {},
   "source": [
    "## Phase 1.5: Generate Pseudo Labels for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042fe73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_pseudo_labels(model, loader, device, scaler):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Generating Pseudo Labels'):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            imgs = batch[0]\n",
    "        else:\n",
    "            imgs = batch\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    # Inverse transform\n",
    "    all_preds = scaler.inverse_transform(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c73a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "test_wide = prepare_data(test_df, is_train=False)\n",
    "print(f\"Test data: {len(test_wide)} images\")\n",
    "\n",
    "# Create test dataset for pseudo labeling\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, df, cfg, transforms):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        return img\n",
    "\n",
    "test_dataset = SimpleImageDataset(test_wide, cfg, get_transforms('val', cfg.input_size))\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "# Load tabular predictor\n",
    "ckpt = torch.load(cfg.OUTPUT_DIR / 'tabular_predictor.pt', weights_only=False)\n",
    "tabular_model = TabularPredictor(cfg.backbone, n_outputs=2, pretrained=False).to(cfg.device)\n",
    "tabular_model.load_state_dict(ckpt['model_state_dict'])\n",
    "scaler = ckpt['scaler']\n",
    "\n",
    "# Generate pseudo labels\n",
    "pseudo_tabular = generate_pseudo_labels(tabular_model, test_loader, cfg.device, scaler)\n",
    "print(f\"Pseudo labels shape: {pseudo_tabular.shape}\")\n",
    "print(f\"  NDVI: min={pseudo_tabular[:, 0].min():.2f}, max={pseudo_tabular[:, 0].max():.2f}\")\n",
    "print(f\"  Height: min={pseudo_tabular[:, 1].min():.2f}, max={pseudo_tabular[:, 1].max():.2f}\")\n",
    "\n",
    "# Add to test_wide\n",
    "test_wide['Pre_GSHH_NDVI'] = pseudo_tabular[:, 0]\n",
    "test_wide['Height_Ave_cm'] = pseudo_tabular[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52633c7f",
   "metadata": {},
   "source": [
    "## Phase 2: Train Biomass Model with Tabular (using pseudo labels for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5d9b5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2, 'Dry_Total_g': 0.5,\n",
    "}\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "def competition_metric(y_true, y_pred):\n",
    "    weights = np.array([TARGET_WEIGHTS[t] for t in TARGET_ORDER])\n",
    "    y_weighted_mean = sum(y_true[:, i].mean() * weights[i] for i in range(5))\n",
    "    ss_res = sum(((y_true[:, i] - y_pred[:, i]) ** 2).mean() * weights[i] for i in range(5))\n",
    "    ss_tot = sum(((y_true[:, i] - y_weighted_mean) ** 2).mean() * weights[i] for i in range(5))\n",
    "    return 1 - ss_res / (ss_tot + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730674c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassDatasetWithTabular(Dataset):\n",
    "    \"\"\"Image + Tabular ‚Üí Biomass\"\"\"\n",
    "    def __init__(self, df, cfg, transforms=None, mode='train', tabular_scaler=None):\n",
    "        # Add dummy targets for test BEFORE reset\n",
    "        df = df.copy()\n",
    "        for t in TARGET_ORDER:\n",
    "            if t not in df.columns:\n",
    "                df[t] = 0.0\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Tabular features\n",
    "        if all(col in df.columns for col in cfg.tabular_cols):\n",
    "            tabular_data = df[cfg.tabular_cols].values.astype(np.float32)\n",
    "            if tabular_scaler is not None:\n",
    "                if mode == 'train':\n",
    "                    self.tabular = tabular_scaler.fit_transform(tabular_data)\n",
    "                else:\n",
    "                    self.tabular = tabular_scaler.transform(tabular_data)\n",
    "            else:\n",
    "                self.tabular = tabular_data\n",
    "            self.has_tabular = True\n",
    "        else:\n",
    "            self.tabular = None\n",
    "            self.has_tabular = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_path = self.cfg.DATA_PATH / row['image_path']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        targets = torch.tensor([\n",
    "            row['Dry_Green_g'], row['Dry_Clover_g'], row['Dry_Dead_g']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        if self.has_tabular:\n",
    "            tabular = torch.tensor(self.tabular[idx], dtype=torch.float32)\n",
    "            return img, tabular, targets\n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb8b9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhysicsConstrainedHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 3)\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raw = self.head(x)\n",
    "        independent = self.softplus(raw)\n",
    "        green, clover, dead = independent[:, 0:1], independent[:, 1:2], independent[:, 2:3]\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "        full = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "        return independent, full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a498f4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BiomassModelWithTabular(nn.Module):\n",
    "    \"\"\"FiLM Conditioning: Image + Tabular ‚Üí Biomass\"\"\"\n",
    "    def __init__(self, backbone_name, n_tabular=2, pretrained=True, weights_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained and weights_path and Path(weights_path).exists():\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "            weights = torch.load(weights_path, weights_only=True)\n",
    "            weights = {k: v for k, v in weights.items() if not k.startswith('classifier')}\n",
    "            self.backbone.load_state_dict(weights, strict=False)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        feat_dim = self.backbone.num_features\n",
    "        \n",
    "        # Tabular encoder + FiLM\n",
    "        self.tabular_encoder = nn.Sequential(\n",
    "            nn.Linear(n_tabular, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.film_gamma = nn.Linear(128, feat_dim)\n",
    "        self.film_beta = nn.Linear(128, feat_dim)\n",
    "        \n",
    "        self.head = PhysicsConstrainedHead(feat_dim)\n",
    "    \n",
    "    def forward(self, image, tabular=None):\n",
    "        img_feat = self.backbone(image)\n",
    "        \n",
    "        if tabular is not None:\n",
    "            tab_feat = self.tabular_encoder(tabular)\n",
    "            gamma = self.film_gamma(tab_feat)\n",
    "            beta = self.film_beta(tab_feat)\n",
    "            img_feat = img_feat * (1 + gamma) + beta\n",
    "        \n",
    "        return self.head(img_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145ba53",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_biomass_with_tabular(train_df, cfg):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üåø Phase 2: Training Biomass Model with Tabular\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_data = train_df[train_df['fold'] != 0].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['fold'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    tabular_scaler = StandardScaler()\n",
    "    \n",
    "    train_dataset = BiomassDatasetWithTabular(\n",
    "        train_data, cfg, get_transforms('train', cfg.input_size), 'train', tabular_scaler\n",
    "    )\n",
    "    val_dataset = BiomassDatasetWithTabular(\n",
    "        val_data, cfg, get_transforms('val', cfg.input_size), 'val', tabular_scaler\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=cfg.num_workers)\n",
    "    \n",
    "    weights_path = None\n",
    "    if cfg.WEIGHTS_PATH.exists():\n",
    "        weights_path = str(cfg.WEIGHTS_PATH / cfg.backbone / f\"{cfg.backbone}.pth\")\n",
    "    \n",
    "    model = BiomassModelWithTabular(cfg.backbone, n_tabular=2, pretrained=True, weights_path=weights_path)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(cfg.device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, tabular, targets in tqdm(train_loader, desc='Train', leave=False):\n",
    "            imgs = imgs.to(cfg.device)\n",
    "            tabular = tabular.to(cfg.device)\n",
    "            targets = targets.to(cfg.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            independent, _ = model(imgs, tabular)\n",
    "            loss = F.mse_loss(independent, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, tabular, targets in val_loader:\n",
    "                imgs = imgs.to(cfg.device)\n",
    "                tabular = tabular.to(cfg.device)\n",
    "                _, full_pred = model(imgs, tabular)\n",
    "                all_preds.append(full_pred.cpu().numpy())\n",
    "                \n",
    "                green, clover, dead = targets[:, 0:1], targets[:, 1:2], targets[:, 2:3]\n",
    "                gdm = green + clover\n",
    "                total = gdm + dead\n",
    "                full_targets = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "                all_targets.append(full_targets.numpy())\n",
    "        \n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        cv_score = competition_metric(all_targets, all_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{cfg.epochs} | Train: {train_loss:.2f} | CV: {cv_score:.4f}\")\n",
    "        \n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'score': best_score,\n",
    "                'tabular_scaler': tabular_scaler,\n",
    "            }, cfg.OUTPUT_DIR / 'exp_b_biomass.pt')\n",
    "            print(f\"  ‚úì Saved!\")\n",
    "    \n",
    "    flush()\n",
    "    return best_score, tabular_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79af690",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv, biomass_tabular_scaler = train_biomass_with_tabular(train_wide, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c505d81",
   "metadata": {},
   "source": [
    "## Phase 3: Inference with Pseudo Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset with pseudo tabular\n",
    "test_dataset = BiomassDatasetWithTabular(\n",
    "    test_wide, cfg, get_transforms('val', cfg.input_size), 'test', biomass_tabular_scaler\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "# Load model\n",
    "ckpt = torch.load(cfg.OUTPUT_DIR / 'exp_b_biomass.pt', weights_only=False)\n",
    "model = BiomassModelWithTabular(cfg.backbone, n_tabular=2, pretrained=False).to(cfg.device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Loaded model (CV: {ckpt['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8252e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Inference'):\n",
    "        if len(batch) == 3:\n",
    "            imgs, tabular, _ = batch\n",
    "            imgs = imgs.to(cfg.device)\n",
    "            tabular = tabular.to(cfg.device)\n",
    "            _, full_pred = model(imgs, tabular)\n",
    "        else:\n",
    "            imgs, _ = batch\n",
    "            imgs = imgs.to(cfg.device)\n",
    "            _, full_pred = model(imgs, None)\n",
    "        all_preds.append(full_pred.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(all_preds)\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1560933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "def melt_table(df):\n",
    "    melted = df.melt(\n",
    "        id_vars='image_path', value_vars=TARGET_ORDER,\n",
    "        var_name='target_name', value_name='target'\n",
    "    )\n",
    "    melted['sample_id'] = (\n",
    "        melted['image_path']\n",
    "        .str.replace(r'^.*/', '', regex=True)\n",
    "        .str.replace('.jpg', '', regex=False)\n",
    "        + '__' + melted['target_name']\n",
    "    )\n",
    "    return melted[['sample_id', 'target']]\n",
    "\n",
    "test_wide[TARGET_ORDER] = preds\n",
    "test_wide[TARGET_ORDER] = test_wide[TARGET_ORDER].clip(lower=0)\n",
    "\n",
    "submission = melt_table(test_wide)\n",
    "submission.to_csv(cfg.OUTPUT_DIR / 'submission_exp_b.csv', index=False)\n",
    "\n",
    "print(f\"\\nüìÑ Submission saved: {len(submission)} rows\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "üî¨ Exp B: Pseudo-Labeling ÏôÑÎ£å\n",
    "{'='*60}\n",
    "\n",
    "CV Score: {best_cv:.4f} (1-fold quick test)\n",
    "\n",
    "Pipeline:\n",
    "1. Image ‚Üí NDVI/Height ÏòàÏ∏° Î™®Îç∏ ÌïôÏäµ\n",
    "2. Test Ïù¥ÎØ∏ÏßÄÏóê pseudo NDVI/Height ÏÉùÏÑ±\n",
    "3. Image + Pseudo TabularÎ°ú Biomass ÏòàÏ∏°\n",
    "\n",
    "Output: {cfg.OUTPUT_DIR / 'submission_exp_b.csv'}\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
