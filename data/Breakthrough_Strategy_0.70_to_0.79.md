# ğŸš€ Breakthrough Strategy: 0.70 â†’ 0.79+

## ğŸ“Š í˜„ì¬ ìƒí™© ë¶„ì„

### í˜„ì¬ ì ìˆ˜
- **Your Best Public LB**: 0.70
- **1ìœ„ Public LB**: 0.79
- **Gap**: 0.09 (ìƒë‹¹íˆ í° ì°¨ì´)

### í˜„ì¬ ì½”ë“œ ë¶„ì„

| Version | íŠ¹ì§• | ë¬¸ì œì  |
|---------|------|--------|
| v20/v26 | DINOv3 Large + FiLM + Dual View | ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸ |
| v22 | Frozen backbone + ì‘ì€ Head | ì œí•œëœ í•™ìŠµ |
| v25 | VegIdx Late Fusion | ì¶”ê°€ ì •ë³´ì§€ë§Œ íš¨ê³¼ ì œí•œì  |
| v27 | ë‹¨ìˆœ ì•™ìƒë¸” (Simple/Rank Average) | ìµœì í™”ë˜ì§€ ì•Šì€ ì•™ìƒë¸” |

### ğŸ”´ í•µì‹¬ ë¬¸ì œì  ë°œê²¬

#### 1. **CV ì „ëµ ì˜¤ë¥˜** âš ï¸ (ê°€ì¥ ì‹¬ê°)
```python
# í˜„ì¬ ì½”ë“œ
sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)
df['strat_key'] = df['State'] + '_' + df['Month'].astype(str)
groups = df['image_id']  # image_idë¡œ ê·¸ë£¹í•‘
```

**ë¬¸ì œ**: Discussionì—ì„œ 126 votesë¥¼ ë°›ì€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ëŠ” **Sampling_Dateë¡œ ê·¸ë£¹í•‘**í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ!
- ê°™ì€ ë‚ ì§œì— ì´¬ì˜ëœ ì´ë¯¸ì§€ë“¤ì€ ë¹„ìŠ·í•œ ì¡°ê±´ ê³µìœ 
- `image_id`ë¡œ ê·¸ë£¹í•‘í•˜ë©´ ê°™ì€ ë‚ ì§œì˜ ë‹¤ë¥¸ ì´ë¯¸ì§€ê°€ train/valì— ë¶„ë¦¬ë¨
- **ì‹¬ê°í•œ data leakage â†’ overfitting**

#### 2. **ì´ë¯¸ì§€ í•´ìƒë„ ì œí•œ**
```python
img_size = (512, 512)
```
- DINOv2/v3ì˜ ìµœì  í•´ìƒë„ëŠ” **518x518** (14ë¡œ ë‚˜ëˆ ë–¨ì–´ì§)
- ë˜ëŠ” ë” í° í•´ìƒë„ (560, 616, 672 ë“±)

#### 3. **TTA ë¯¸ì‚¬ìš©**
- Inferenceì—ì„œ TTAë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- ëª©ì´ˆì§€ ì´ë¯¸ì§€ëŠ” íšŒì „/í”Œë¦½ì— ë¶ˆë³€ â†’ TTA íš¨ê³¼ì 

#### 4. **ì•™ìƒë¸” ìµœì í™” ë¶€ì¡±**
```python
ENSEMBLE_METHOD = "simple"  # ë‹¨ìˆœ í‰ê· ë§Œ ì‚¬ìš©
```
- ê°€ì¤‘ì¹˜ ìµœì í™” ì—†ìŒ
- ëª¨ë¸ ë‹¤ì–‘ì„± ë¶€ì¡± (ëª¨ë‘ ê°™ì€ backbone)

#### 5. **Loss Function**
```python
main_loss = F.mse_loss(pred, main_targets)  # ë‹¨ìˆœ MSE
```
- ëŒ€íšŒ í‰ê°€ ì§€í‘œ(Weighted RÂ²)ì™€ ë‹¤ë¥¸ loss ì‚¬ìš©
- Dry_Total_gê°€ 50% ê°€ì¤‘ì¹˜ì¸ë° ë™ì¼í•˜ê²Œ ì·¨ê¸‰

#### 6. **ë°ì´í„° í™œìš© ë¶€ì¡±**
- External data (Irish Grass Clover) ë¯¸ì‚¬ìš©
- Pseudo labeling ë¯¸ì ìš©

---

## ğŸ¯ Breakthrough ì „ëµ (ìš°ì„ ìˆœìœ„ ìˆœ)

### ğŸ”¥ Priority 1: CV ì „ëµ ìˆ˜ì • (ì˜ˆìƒ +0.03~0.05)

**ê°€ì¥ ì¤‘ìš”!! ì´ê²ƒë§Œ ê³ ì³ë„ í° í–¥ìƒ ì˜ˆìƒ**

```python
# âŒ í˜„ì¬ (ì˜ëª»ëœ ë°©ë²•)
groups = df['image_id']

# âœ… ìˆ˜ì • (ì˜¬ë°”ë¥¸ ë°©ë²•)
groups = df['Sampling_Date']  # ë‚ ì§œë³„ ê·¸ë£¹í•‘!

# ë˜ëŠ” ë” ë³´ìˆ˜ì ìœ¼ë¡œ
df['date_group'] = pd.to_datetime(df['Sampling_Date']).dt.strftime('%Y-%m-%d')
groups = df['date_group']
```

```python
def create_proper_folds(df, n_splits=5):
    """Sampling_Date ê¸°ë°˜ ì˜¬ë°”ë¥¸ CV split"""
    df = df.copy()

    # Sampling_Dateë¥¼ ê·¸ë£¹ìœ¼ë¡œ ì‚¬ìš©
    df['date_group'] = pd.to_datetime(df['Sampling_Date']).dt.strftime('%Y-%m-%d')

    # State + Monthë¡œ stratify (ì„ íƒì )
    df['strat_key'] = df['State'] + '_' + df['Month'].astype(str)

    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)

    df['fold'] = -1
    for fold, (_, val_idx) in enumerate(sgkf.split(
        df,
        df['strat_key'],
        groups=df['date_group']  # âš ï¸ í•µì‹¬: dateë¡œ ê·¸ë£¹í•‘!
    )):
        df.loc[val_idx, 'fold'] = fold

    return df
```

### ğŸ”¥ Priority 2: ë” í° í•´ìƒë„ + TTA (ì˜ˆìƒ +0.02~0.03)

```python
# í•´ìƒë„ ë³€ê²½
img_size = (518, 518)  # DINOv2/v3 ìµœì 
# ë˜ëŠ”
img_size = (560, 560)  # ë” í° í•´ìƒë„

# TTA êµ¬í˜„
def tta_predict(model, left, right, device, n_tta=8):
    """Test-Time Augmentation"""
    preds = []

    augmentations = [
        (False, False, 0),   # Original
        (True, False, 0),    # H-flip
        (False, True, 0),    # V-flip
        (True, True, 0),     # HV-flip
        (False, False, 1),   # 90Â°
        (False, False, 2),   # 180Â°
        (False, False, 3),   # 270Â°
        (True, False, 1),    # H-flip + 90Â°
    ]

    for hflip, vflip, rot in augmentations[:n_tta]:
        l, r = left.clone(), right.clone()

        if hflip:
            l = torch.flip(l, [3])
            r = torch.flip(r, [3])
        if vflip:
            l = torch.flip(l, [2])
            r = torch.flip(r, [2])
        if rot > 0:
            l = torch.rot90(l, rot, [2, 3])
            r = torch.rot90(r, rot, [2, 3])

        with torch.no_grad():
            pred = model(l.to(device), r.to(device))
            preds.append(pred.cpu())

    return torch.stack(preds).mean(0)
```

### ğŸ”¥ Priority 3: Weighted Loss (ì˜ˆìƒ +0.01~0.02)

```python
class WeightedR2Loss(nn.Module):
    """ëŒ€íšŒ í‰ê°€ ì§€í‘œì— ë§ì¶˜ Loss"""
    def __init__(self):
        super().__init__()
        # [Green, Dead, Clover, GDM, Total]
        self.weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])

    def forward(self, pred, target):
        # Component loss (Green, Clover, Dead ì˜ˆì¸¡)
        component_pred = pred[:, [0, 2, 1]]  # Green, Clover, Dead
        component_target = target[:, :3]

        # 5ê°œ íƒ€ê²Ÿ êµ¬ì„±
        green, clover, dead = pred[:, 0:1], pred[:, 2:3], pred[:, 1:2]
        gdm_pred = green + clover
        total_pred = gdm_pred + dead

        full_pred = torch.cat([green, dead, clover, gdm_pred, total_pred], dim=1)

        # ê°€ì¤‘ MSE (Dry_Total_gì— 50% ê°€ì¤‘ì¹˜!)
        weights = self.weights.to(pred.device)
        mse = (full_pred - target) ** 2
        weighted_mse = (mse * weights).mean()

        return weighted_mse

# ë˜ëŠ” ë” ë‹¨ìˆœí•˜ê²Œ: Dry_Total_gì— ì¶”ê°€ Loss
def total_focused_loss(pred, target, alpha=0.5):
    """Dry_Total_g ì¤‘ì‹¬ Loss"""
    component_loss = F.mse_loss(pred[:, :3], target[:, :3])
    total_loss = F.mse_loss(pred[:, 4], target[:, 4])  # Total
    return component_loss + alpha * total_loss
```

### ğŸ”¥ Priority 4: Multi-Resolution Ensemble (ì˜ˆìƒ +0.01~0.02)

```python
# ë‹¤ì–‘í•œ í•´ìƒë„ë¡œ í•™ìŠµëœ ëª¨ë¸ ì•™ìƒë¸”
resolutions = [448, 518, 560]
models = []

for res in resolutions:
    cfg.img_size = (res, res)
    model = train_model(cfg)
    models.append(model)

# Inference ì‹œ í‰ê· 
final_pred = np.mean([m.predict(test) for m in models], axis=0)
```

### ğŸ”¥ Priority 5: Seed Ensemble (ì˜ˆìƒ +0.005~0.01)

```python
# ë‹¤ì–‘í•œ seedë¡œ í•™ìŠµ
seeds = [42, 123, 456, 789, 1024]

all_preds = []
for seed in seeds:
    seed_everything(seed)
    model = train_model(cfg)
    all_preds.append(model.predict(test))

final_pred = np.mean(all_preds, axis=0)
```

### ğŸ”¥ Priority 6: ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (ì˜ˆìƒ +0.01)

```python
from scipy.optimize import minimize

def optimize_ensemble_weights(oof_preds_list, oof_targets):
    """OOF ê¸°ë°˜ ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì°¾ê¸°"""
    n_models = len(oof_preds_list)

    def objective(weights):
        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
        weights = np.abs(weights)  # ì–‘ìˆ˜ ë³´ì¥
        weights = weights / weights.sum()  # í•©ì´ 1

        ensemble_pred = sum(w * p for w, p in zip(weights, oof_preds_list))

        # Negative RÂ² (ìµœì†Œí™” ëª©ì )
        return -competition_metric(oof_targets, ensemble_pred)

    # ì´ˆê¸°ê°’: ê· ë“± ê°€ì¤‘ì¹˜
    x0 = np.ones(n_models) / n_models

    result = minimize(objective, x0, method='Nelder-Mead')

    optimal_weights = np.abs(result.x)
    optimal_weights = optimal_weights / optimal_weights.sum()

    return optimal_weights

# ì‚¬ìš©
optimal_weights = optimize_ensemble_weights(
    [oof_v20, oof_v22, oof_v25, oof_v26],
    oof_targets
)
print(f"Optimal weights: {optimal_weights}")
```

---

## ğŸ“… 9ì¼ ì‹¤í–‰ ê³„íš

### Day 1-2: CV ìˆ˜ì • + ì¬í•™ìŠµ (ê°€ì¥ ì¤‘ìš”!)
```
1. Sampling_Date ê¸°ë°˜ GroupKFoldë¡œ ë³€ê²½
2. ê¸°ì¡´ v20 ì•„í‚¤í…ì²˜ë¡œ ì¬í•™ìŠµ
3. ìƒˆë¡œìš´ CV ì ìˆ˜ í™•ì¸ (Local CVê°€ LBì™€ ë” ì¼ì¹˜í•´ì•¼ í•¨)
```

### Day 3-4: í•´ìƒë„ + TTA ì‹¤í—˜
```
1. img_size = (518, 518) ë˜ëŠ” (560, 560)ë¡œ ë³€ê²½
2. TTA êµ¬í˜„ ë° ì ìš©
3. ì œì¶œ ë° LB í™•ì¸
```

### Day 5-6: Loss í•¨ìˆ˜ + ë‹¤ì–‘ì„±
```
1. Weighted Loss ì ìš©
2. ë‹¤ë¥¸ seedë¡œ ì¶”ê°€ ëª¨ë¸ í•™ìŠµ
3. Multi-resolution ì‹¤í—˜
```

### Day 7-8: ì•™ìƒë¸” ìµœì í™”
```
1. OOF ê¸°ë°˜ ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°
2. ë‹¤ì–‘í•œ ì•™ìƒë¸” ì¡°í•© ì‹¤í—˜
3. Blending ë˜ëŠ” Stacking ì‹œë„
```

### Day 9: ìµœì¢… ì œì¶œ
```
1. ìµœì  ì¡°í•© ì„ íƒ
2. ì•ˆì „í•œ ë°±ì—… ì œì¶œ
3. Final submission
```

---

## ğŸ”§ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì½”ë“œ ìˆ˜ì •

### 1. CV ìˆ˜ì • (v20/v26 ê¸°ë°˜)

```python
def create_proper_folds(df, n_splits=5):
    """âš ï¸ í•µì‹¬ ìˆ˜ì •: Sampling_Date ê¸°ë°˜ CV"""
    df = df.copy()

    # ë‚ ì§œ ê·¸ë£¹ ìƒì„±
    df['date_group'] = pd.to_datetime(df['Sampling_Date']).dt.strftime('%Y-%m-%d')

    # Stratification key (State + Month)
    df['strat_key'] = df['State'] + '_' + df['Month'].astype(str)

    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)

    df['fold'] = -1
    for fold, (_, val_idx) in enumerate(sgkf.split(
        df,
        df['strat_key'],
        groups=df['date_group']  # âš ï¸ dateë¡œ ê·¸ë£¹í•‘!
    )):
        df.loc[val_idx, 'fold'] = fold

    # ê²€ì¦: ê°™ì€ ë‚ ì§œê°€ ì—¬ëŸ¬ foldì— ìˆìœ¼ë©´ ì•ˆ ë¨
    date_fold_counts = df.groupby('date_group')['fold'].nunique()
    if (date_fold_counts > 1).any():
        print("âš ï¸ WARNING: Some dates are in multiple folds!")
    else:
        print("âœ“ CV split verified: dates are properly grouped")

    return df
```

### 2. í•´ìƒë„ ë³€ê²½

```python
class CFG:
    img_size = (518, 518)  # 512 â†’ 518 (DINOv2 ìµœì )
```

### 3. TTA ì¶”ê°€ (Inference)

```python
@torch.no_grad()
def predict_with_tta(model, left, right, device, n_tta=4):
    """ê°„ë‹¨í•œ TTA: flip 4ê°€ì§€"""
    preds = []

    for hflip in [False, True]:
        for vflip in [False, True]:
            l = torch.flip(left, [3]) if hflip else left
            r = torch.flip(right, [3]) if hflip else right
            l = torch.flip(l, [2]) if vflip else l
            r = torch.flip(r, [2]) if vflip else r

            pred = model(l.to(device), r.to(device))
            preds.append(pred.cpu())

    return torch.stack(preds).mean(0)
```

---

## ğŸ“Š ì˜ˆìƒ ê°œì„  íš¨ê³¼

| ì „ëµ | ì˜ˆìƒ í–¥ìƒ | ë‚œì´ë„ | ìš°ì„ ìˆœìœ„ |
|------|----------|--------|----------|
| CV ìˆ˜ì • (Sampling_Date) | +0.03~0.05 | ì‰¬ì›€ | **1 (í•„ìˆ˜!)** |
| í•´ìƒë„ 518 | +0.01 | ì‰¬ì›€ | 2 |
| TTA (4-fold) | +0.01~0.02 | ì‰¬ì›€ | 3 |
| Weighted Loss | +0.01~0.02 | ì¤‘ê°„ | 4 |
| Multi-seed | +0.005~0.01 | ì‰¬ì›€ | 5 |
| ì•™ìƒë¸” ìµœì í™” | +0.01 | ì¤‘ê°„ | 6 |

**ì´ ì˜ˆìƒ í–¥ìƒ: +0.07~0.12 â†’ 0.77~0.82 ê°€ëŠ¥!**

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **CV-LB Correlation í™•ì¸**
   - CV ìˆ˜ì • í›„ Local CVì™€ LBì˜ ìƒê´€ê´€ê³„ í™•ì¸
   - ìƒê´€ê´€ê³„ê°€ ë†’ì•„ì•¼ ì‹ ë¢°í•  ìˆ˜ ìˆìŒ

2. **Overfitting ì£¼ì˜**
   - 357ê°œ ì´ë¯¸ì§€ë¡œ ì‘ì€ ë°ì´í„°ì…‹
   - ë„ˆë¬´ ë³µì¡í•œ ëª¨ë¸/ì•™ìƒë¸”ì€ ì˜¤íˆë ¤ í•´ë¡œìš¸ ìˆ˜ ìˆìŒ

3. **Private LB ëŒ€ë¹„**
   - Public 53% / Private 47% ë¶„í• 
   - ê³¼ë„í•œ LB probing í”¼í•˜ê¸°

---

*Created: 2026-01-19*
*Target: 0.70 â†’ 0.79+*
*Most Important: Fix CV strategy with Sampling_Date grouping!*
