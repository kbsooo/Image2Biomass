# ğŸ¯ Breakthrough ì „ëµ: í›ˆë ¨ vs ì¶”ë¡  ë¶„ë¥˜

## ğŸ“š í›ˆë ¨(Training) ë‹¨ê³„ì—ì„œ í•  ê²ƒ

### 1. âš ï¸ CV ì „ëµ ìˆ˜ì • (ê°€ì¥ ì¤‘ìš”!)
**ì˜ˆìƒ í–¥ìƒ: +0.03~0.05**

```python
# âŒ í˜„ì¬
groups = df['image_id']

# âœ… ìˆ˜ì •
groups = df['Sampling_Date']  # ë‚ ì§œë³„ ê·¸ë£¹í•‘
```

- ëª¨ë¸ì„ **ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµ**í•´ì•¼ í•¨
- CVê°€ ë°”ë€Œë©´ ëª¨ë“  foldì˜ train/val ë¶„í• ì´ ë‹¬ë¼ì§

---

### 2. í•´ìƒë„ ë³€ê²½
**ì˜ˆìƒ í–¥ìƒ: +0.01**

```python
# âŒ í˜„ì¬
img_size = (512, 512)

# âœ… ìˆ˜ì •
img_size = (518, 518)  # ë˜ëŠ” (560, 560)
```

- ë‹¤ë¥¸ í•´ìƒë„ë¡œ **ì¬í•™ìŠµ í•„ìš”**
- ì¶”ë¡  ì‹œì—ë„ ê°™ì€ í•´ìƒë„ ì‚¬ìš©í•´ì•¼ í•¨

---

### 3. Weighted Loss ì ìš©
**ì˜ˆìƒ í–¥ìƒ: +0.01~0.02**

```python
# âŒ í˜„ì¬
main_loss = F.mse_loss(pred, main_targets)  # ë‹¨ìˆœ MSE

# âœ… ìˆ˜ì •: Dry_Total_gì— 50% ê°€ì¤‘ì¹˜ ë°˜ì˜
weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])  # Green, Dead, Clover, GDM, Total
weighted_loss = (mse * weights).mean()
```

- Loss í•¨ìˆ˜ ë³€ê²½ â†’ **ì¬í•™ìŠµ í•„ìš”**

---

### 4. Multi-Seed í•™ìŠµ
**ì˜ˆìƒ í–¥ìƒ: +0.005~0.01**

```python
seeds = [42, 123, 456, 789, 1024]
for seed in seeds:
    seed_everything(seed)
    train_model(cfg)  # ê° seedë¡œ ë³„ë„ í•™ìŠµ
```

- ê°™ì€ ì„¤ì •, ë‹¤ë¥¸ seedë¡œ **ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ**
- ì¶”ë¡  ì‹œ ì•™ìƒë¸”ë¡œ ì‚¬ìš©

---

### 5. Multi-Resolution í•™ìŠµ
**ì˜ˆìƒ í–¥ìƒ: +0.01~0.02**

```python
resolutions = [448, 518, 560]
for res in resolutions:
    cfg.img_size = (res, res)
    train_model(cfg)  # ê° í•´ìƒë„ë¡œ ë³„ë„ í•™ìŠµ
```

- ë‹¤ì–‘í•œ í•´ìƒë„ë¡œ **ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ**
- ì¶”ë¡  ì‹œ ì•™ìƒë¸”ë¡œ ì‚¬ìš©

---

### 6. Data Augmentation ê°•í™” (ì„ íƒì )
**ì˜ˆìƒ í–¥ìƒ: +0.005~0.01**

```python
# ë” ê°•í•œ augmentation
T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05)
T.RandomRotation(degrees=15)
T.RandomAffine(degrees=0, translate=(0.1, 0.1))
```

- Augmentation ë³€ê²½ â†’ **ì¬í•™ìŠµ í•„ìš”**

---

## ğŸ”® ì¶”ë¡ (Inference) ë‹¨ê³„ì—ì„œ í•  ê²ƒ

### 1. TTA (Test-Time Augmentation)
**ì˜ˆìƒ í–¥ìƒ: +0.01~0.02**

```python
@torch.no_grad()
def predict_with_tta(model, left, right, device, n_tta=4):
    """ê¸°ì¡´ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì¶”ë¡ ë§Œ ë³€ê²½"""
    preds = []

    for hflip in [False, True]:
        for vflip in [False, True]:
            l = torch.flip(left, [3]) if hflip else left
            r = torch.flip(right, [3]) if hflip else right
            l = torch.flip(l, [2]) if vflip else l
            r = torch.flip(r, [2]) if vflip else r

            pred = model(l.to(device), r.to(device))
            preds.append(pred.cpu())

    return torch.stack(preds).mean(0)  # í‰ê· 
```

- **ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- ì¶”ë¡  ì½”ë“œë§Œ ìˆ˜ì •í•˜ë©´ ë¨
- ì¶”ë¡  ì‹œê°„ 4~8ë°° ì¦ê°€

---

### 2. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
**ì˜ˆìƒ í–¥ìƒ: +0.01**

```python
# v27_infer.pyì˜ WEIGHTS ìµœì í™”
# âŒ í˜„ì¬: ë‹¨ìˆœ í‰ê·  ë˜ëŠ” ìˆ˜ë™ ê°€ì¤‘ì¹˜
ENSEMBLE_METHOD = "simple"

# âœ… ìˆ˜ì •: OOF ê¸°ë°˜ ìµœì  ê°€ì¤‘ì¹˜
from scipy.optimize import minimize

def find_optimal_weights(oof_preds_list, oof_targets):
    def objective(weights):
        weights = np.abs(weights) / np.abs(weights).sum()
        ensemble = sum(w * p for w, p in zip(weights, oof_preds_list))
        return -competition_metric(oof_targets, ensemble)

    result = minimize(objective, np.ones(len(oof_preds_list)))
    return np.abs(result.x) / np.abs(result.x).sum()

optimal_weights = find_optimal_weights([oof_v20, oof_v22, oof_v25, oof_v26], oof_targets)
```

- **ê¸°ì¡´ ëª¨ë¸ë“¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- OOF ì˜ˆì¸¡ê°’ìœ¼ë¡œ ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚°
- ì¶”ë¡  ì‹œ ê°€ì¤‘ í‰ê·  ì ìš©

---

### 3. Post-Processing
**ì˜ˆìƒ í–¥ìƒ: +0.005~0.01**

```python
# ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬
def post_process(predictions):
    # 1. Negative ê°’ í´ë¦¬í•‘
    predictions = np.maximum(predictions, 0)

    # 2. íƒ€ê²Ÿ ê°„ ì¼ê´€ì„± ë³´ì •
    # Dry_Total â‰ˆ Dry_Clover + Dry_Dead + Dry_Green
    green, dead, clover = predictions[:, 0], predictions[:, 1], predictions[:, 2]
    gdm, total = predictions[:, 3], predictions[:, 4]

    # í•©ê³„ ì¼ê´€ì„± ì²´í¬ ë° ë³´ì •
    sum_components = green + dead + clover
    predictions[:, 4] = (total + sum_components) / 2  # í‰ê· ìœ¼ë¡œ ë³´ì •

    return predictions
```

- **ê¸°ì¡´ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- ì˜ˆì¸¡ ê²°ê³¼ë§Œ í›„ì²˜ë¦¬

---

### 4. Rank Average Ensemble
**ì˜ˆìƒ í–¥ìƒ: +0.005**

```python
# v27_infer.pyì—ì„œ ì´ë¯¸ êµ¬í˜„ë¨
ENSEMBLE_METHOD = "rank"  # simple â†’ rankë¡œ ë³€ê²½
```

- **ê¸°ì¡´ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- ì•™ìƒë¸” ë°©ë²•ë§Œ ë³€ê²½

---

## ğŸ“Š ìš”ì•½ í…Œì´ë¸”

| ì „ëµ | ë‹¨ê³„ | ì˜ˆìƒ í–¥ìƒ | ì¬í•™ìŠµ í•„ìš” | ë‚œì´ë„ |
|------|------|----------|------------|--------|
| **CV ìˆ˜ì • (Sampling_Date)** | ğŸ‹ï¸ í›ˆë ¨ | +0.03~0.05 | âœ… í•„ìˆ˜ | ì‰¬ì›€ |
| í•´ìƒë„ 518x518 | ğŸ‹ï¸ í›ˆë ¨ | +0.01 | âœ… í•„ìˆ˜ | ì‰¬ì›€ |
| Weighted Loss | ğŸ‹ï¸ í›ˆë ¨ | +0.01~0.02 | âœ… í•„ìˆ˜ | ì¤‘ê°„ |
| Multi-Seed | ğŸ‹ï¸ í›ˆë ¨ | +0.005~0.01 | âœ… í•„ìˆ˜ | ì‰¬ì›€ |
| Multi-Resolution | ğŸ‹ï¸ í›ˆë ¨ | +0.01~0.02 | âœ… í•„ìˆ˜ | ì‰¬ì›€ |
| **TTA** | ğŸ”® ì¶”ë¡  | +0.01~0.02 | âŒ ë¶ˆí•„ìš” | ì‰¬ì›€ |
| **ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”** | ğŸ”® ì¶”ë¡  | +0.01 | âŒ ë¶ˆí•„ìš” | ì¤‘ê°„ |
| **Post-Processing** | ğŸ”® ì¶”ë¡  | +0.005~0.01 | âŒ ë¶ˆí•„ìš” | ì‰¬ì›€ |
| Rank Average | ğŸ”® ì¶”ë¡  | +0.005 | âŒ ë¶ˆí•„ìš” | ì‰¬ì›€ |

---

## âš¡ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê²ƒ (ì¶”ë¡ ë§Œ ìˆ˜ì •)

**ì˜¤ëŠ˜ ë°”ë¡œ ì‹œë„ ê°€ëŠ¥ (ê¸°ì¡´ v20/v22/v25/v26 ëª¨ë¸ ì‚¬ìš©):**

1. **TTA ì¶”ê°€** â†’ v27_infer.py ìˆ˜ì •
2. **Rank Average ì‚¬ìš©** â†’ `ENSEMBLE_METHOD = "rank"`
3. **ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”** â†’ OOF ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
4. **Post-Processing** â†’ ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬

```python
# v27_infer.py ìˆ˜ì • ì˜ˆì‹œ

# 1. TTA í•¨ìˆ˜ ì¶”ê°€
@torch.no_grad()
def predict_with_tta(model, loader, device):
    model.eval()
    all_outputs, all_ids = [], []

    for left, right, ids in tqdm(loader):
        # 4-way TTA (original + 3 flips)
        preds = []
        for hf in [False, True]:
            for vf in [False, True]:
                l = torch.flip(left, [3]) if hf else left
                r = torch.flip(right, [3]) if hf else right
                l = torch.flip(l, [2]) if vf else l
                r = torch.flip(r, [2]) if vf else r

                out = model(l.to(device), r.to(device))
                preds.append(out.cpu())

        avg_pred = torch.stack(preds).mean(0)
        all_outputs.append(avg_pred.numpy())
        all_ids.extend(ids)

    return np.concatenate(all_outputs), all_ids

# 2. ì•™ìƒë¸” ë°©ë²• ë³€ê²½
ENSEMBLE_METHOD = "rank"  # ë˜ëŠ” ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©

# 3. Post-processing ì¶”ê°€
final_preds = np.maximum(final_preds, 0)  # ìŒìˆ˜ ì œê±°
```

---

## ğŸ¯ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ

### Phase 1: ì¦‰ì‹œ (ì¶”ë¡ ë§Œ ìˆ˜ì •) - ì˜¤ëŠ˜
```
1. v27_infer.pyì— TTA ì¶”ê°€
2. ENSEMBLE_METHOD = "rank" ë³€ê²½
3. Post-processing ì¶”ê°€
4. ì œì¶œ â†’ LB í™•ì¸
```
**ì˜ˆìƒ: 0.70 â†’ 0.71~0.72**

### Phase 2: ë‹¨ê¸° (ì¬í•™ìŠµ í•„ìš”) - 1~3ì¼
```
1. CV ìˆ˜ì • (Sampling_Date ê·¸ë£¹í•‘) â† ê°€ì¥ ì¤‘ìš”!
2. í•´ìƒë„ 518x518ë¡œ ë³€ê²½
3. ì¬í•™ìŠµ ë° ì œì¶œ
```
**ì˜ˆìƒ: 0.71 â†’ 0.75~0.77**

### Phase 3: ì¤‘ê¸° (ì¶”ê°€ ìµœì í™”) - 4~7ì¼
```
1. Weighted Loss ì ìš©
2. Multi-seed í•™ìŠµ
3. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
```
**ì˜ˆìƒ: 0.77 â†’ 0.79+**

---

*ê²°ë¡ : CV ìˆ˜ì •(í›ˆë ¨)ì´ ê°€ì¥ í° í–¥ìƒì„ ê°€ì ¸ì˜¤ì§€ë§Œ, TTA(ì¶”ë¡ )ëŠ” ì˜¤ëŠ˜ ë°”ë¡œ ì ìš© ê°€ëŠ¥!*
