# CSIRO Biomass Competition Strategy
## DINOv2 Large ê¸°ë°˜ ì ‘ê·¼ë²•

---

## ğŸ“‹ ëŒ€íšŒ í•µì‹¬ ì •ë³´ ìš”ì•½

### í‰ê°€ ì§€í‘œ: Weighted RÂ² Score
| Target | Weight | íŠ¹ì„± |
|--------|--------|------|
| **Dry_Total_g** | **0.5** | ê°€ì¥ ì¤‘ìš”! ì „ì²´ ë°”ì´ì˜¤ë§¤ìŠ¤ |
| **GDM_g** | **0.2** | Green Dry Matter |
| Dry_Green_g | 0.1 | ë…¹ìƒ‰ ì‹ë¬¼ |
| Dry_Dead_g | 0.1 | ì£½ì€ ì‹ë¬¼ |
| Dry_Clover_g | 0.1 | í´ë¡œë²„ (37.8% zero) |

### í˜„ì¬ ìƒí™©
- **1ìœ„ ì ìˆ˜**: 0.79 RÂ²
- **9ì¼ ë‚¨ìŒ** (ë§ˆê° ì„ë°•)
- **Public LB**: 53% ë°ì´í„° / **Private LB**: 47% ë°ì´í„°
- **Research Code Competition**: ê³µê°œ ë…¸íŠ¸ë¶ ì—†ìŒ

---

## ğŸ”‘ Discussionì—ì„œ ì–»ì€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. âš ï¸ Overfitting ë°©ì§€: Sampling_Dateë¡œ GroupKFold (126 votes)
```
ê°™ì€ ë‚ ì§œì— ì´¬ì˜ëœ ì´ë¯¸ì§€ë“¤ì€ ë¹„ìŠ·í•œ ì¡°ê±´(ë‚ ì”¨, ì¡°ëª… ë“±)ì„ ê³µìœ 
â†’ ë°˜ë“œì‹œ Sampling_Date ê¸°ì¤€ GroupKFold ì‚¬ìš©
â†’ ì¼ë°˜ KFold ì‚¬ìš© ì‹œ ì‹¬ê°í•œ overfitting ë°œìƒ
```

### 2. Height_Ave_cmê³¼ Dead Biomass ê´€ê³„ (79 votes)
- Heightì™€ Dry_Dead_gì˜ ìƒê´€ê´€ê³„ ë¶„ì„
- ë©”íƒ€ë°ì´í„° í™œìš© ê°€ëŠ¥ì„± (ë‹¨, Testì—ëŠ” ì—†ìŒ!)

### 3. Irish Grass Clover Dataset (67 votes)
- ì™¸ë¶€ ë°ì´í„°ì…‹ í™œìš© ê°€ëŠ¥ì„±
- Pre-training ë˜ëŠ” ì¶”ê°€ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©

### 4. Local CV vs LB Gap (41 votes)
- í° gapì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
- CV ì „ëµì´ ë§¤ìš° ì¤‘ìš”

### 5. PCA trick for target dependency (32 votes)
- íƒ€ê²Ÿ ê°„ ì„ í˜• ì¢…ì†ì„± í•´ê²°
- Dry_Total = Dry_Clover + Dry_Dead + Dry_Green ê´€ê³„

### 6. Post-Processing Findings (18 votes)
- ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬ ê¸°ë²• ì¡´ì¬

---

## ğŸ—ï¸ DINOv2 Large ê¸°ë°˜ ëª¨ë¸ ì•„í‚¤í…ì²˜

### DINOv2 ì„ íƒ ì´ìœ 
1. **Self-supervised pretraining**: ìì—° ì´ë¯¸ì§€ì—ì„œ ê°•ë ¥í•œ feature ì¶”ì¶œ
2. **Large ëª¨ë¸**: 1024 dim feature, ë†’ì€ í‘œí˜„ë ¥
3. **Frozen backbone ê°€ëŠ¥**: ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ overfitting ë°©ì§€
4. **Registration token**: ìœ„ì¹˜ ì •ë³´ í™œìš© ê°€ëŠ¥

### ê¶Œì¥ ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Input Image                       â”‚
â”‚              (518 x 518 RGB)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DINOv2 Large (ViT-L/14)                 â”‚
â”‚           Frozen or Fine-tuned                   â”‚
â”‚         Output: [CLS] + Patch tokens             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Feature Aggregation                   â”‚
â”‚   Option 1: [CLS] token only (1024 dim)         â”‚
â”‚   Option 2: [CLS] + Global Avg Pool             â”‚
â”‚   Option 3: Attention Pooling                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Regression Head                       â”‚
â”‚   Linear(1024, 512) â†’ ReLU â†’ Dropout(0.3)       â”‚
â”‚   Linear(512, 256) â†’ ReLU â†’ Dropout(0.2)        â”‚
â”‚   Linear(256, 5) â†’ 5 targets                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì½”ë“œ êµ¬ì¡° ì˜ˆì‹œ
```python
import torch
import torch.nn as nn

class DINOv2BiomassModel(nn.Module):
    def __init__(self, freeze_backbone=True):
        super().__init__()

        # DINOv2 Large backbone
        self.backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')

        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False

        # Regression head
        self.head = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 5)  # 5 targets
        )

    def forward(self, x):
        # Extract features
        features = self.backbone(x)  # [B, 1024]

        # Predict targets
        outputs = self.head(features)
        return outputs
```

---

## ğŸ“Š í•™ìŠµ ì „ëµ

### 1. Cross-Validation ì„¤ì •
```python
from sklearn.model_selection import GroupKFold

# âš ï¸ ë°˜ë“œì‹œ Sampling_Dateë¡œ ê·¸ë£¹í™”!
gkf = GroupKFold(n_splits=5)
groups = train_df.groupby('image_path')['Sampling_Date'].first()

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
    # ê°™ì€ ë‚ ì§œì˜ ì´ë¯¸ì§€ë“¤ì´ train/valì— ë¶„ë¦¬ë˜ì§€ ì•ŠìŒ
    pass
```

### 2. Data Augmentation
```python
import albumentations as A

train_transform = A.Compose([
    A.Resize(518, 518),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.GaussNoise(p=0.2),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

val_transform = A.Compose([
    A.Resize(518, 518),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])
```

### 3. Loss Function
```python
class WeightedMSELoss(nn.Module):
    def __init__(self):
        super().__init__()
        # Competition weights
        self.weights = torch.tensor([0.1, 0.1, 0.1, 0.5, 0.2])
        # [Dry_Clover, Dry_Dead, Dry_Green, Dry_Total, GDM]

    def forward(self, pred, target):
        mse = (pred - target) ** 2
        weighted_mse = (mse * self.weights.to(pred.device)).mean()
        return weighted_mse

# ë˜ëŠ” RÂ² Loss ì§ì ‘ êµ¬í˜„
class R2Loss(nn.Module):
    def forward(self, pred, target, weights):
        ss_res = torch.sum(weights * (target - pred) ** 2)
        ss_tot = torch.sum(weights * (target - torch.mean(target)) ** 2)
        r2 = 1 - ss_res / (ss_tot + 1e-8)
        return 1 - r2  # Lossë¡œ ë³€í™˜
```

### 4. Target Transformation
```python
import numpy as np

# Log1p ë³€í™˜ (ìš°í¸í–¥ ë¶„í¬ ì •ê·œí™”)
y_train_log = np.log1p(y_train)
y_pred = np.expm1(model_pred)  # ì—­ë³€í™˜

# ë˜ëŠ” íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ë§
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
y_train_scaled = scaler.fit_transform(y_train)
```

### 5. Training Configuration
```python
config = {
    'model': 'dinov2_vitl14',
    'image_size': 518,
    'batch_size': 16,  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
    'epochs': 30,
    'lr': 1e-4,  # Headë§Œ í•™ìŠµ ì‹œ
    'lr_backbone': 1e-6,  # Backbone fine-tune ì‹œ
    'weight_decay': 1e-4,
    'scheduler': 'CosineAnnealingWarmRestarts',
    'T_0': 10,
    'T_mult': 2,
    'num_folds': 5,
    'seed': 42
}
```

---

## ğŸš€ ì„±ëŠ¥ í–¥ìƒ ì „ëµ

### Strategy 1: Multi-Scale Input
```python
# ì—¬ëŸ¬ í•´ìƒë„ë¡œ ì˜ˆì¸¡ í›„ ì•™ìƒë¸”
scales = [448, 518, 588]
predictions = []
for scale in scales:
    pred = model(resize(image, scale))
    predictions.append(pred)
final_pred = torch.stack(predictions).mean(dim=0)
```

### Strategy 2: Test-Time Augmentation (TTA)
```python
def tta_predict(model, image, n_aug=8):
    preds = []
    transforms = [
        lambda x: x,                    # Original
        lambda x: torch.flip(x, [2]),   # H-flip
        lambda x: torch.flip(x, [3]),   # V-flip
        lambda x: torch.flip(x, [2,3]), # HV-flip
        lambda x: torch.rot90(x, 1, [2,3]),  # 90Â°
        lambda x: torch.rot90(x, 2, [2,3]),  # 180Â°
        lambda x: torch.rot90(x, 3, [2,3]),  # 270Â°
    ]

    for t in transforms[:n_aug]:
        aug_image = t(image)
        pred = model(aug_image)
        preds.append(pred)

    return torch.stack(preds).mean(dim=0)
```

### Strategy 3: Ensemble
```python
# ë‹¤ì–‘í•œ ëª¨ë¸ ì•™ìƒë¸”
models = [
    'dinov2_vitl14',      # DINOv2 Large
    'dinov2_vitl14_reg',  # DINOv2 Large with registers
    'dinov2_vitg14',      # DINOv2 Giant (ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
]

# Foldë³„ ëª¨ë¸ ì•™ìƒë¸”
final_pred = np.zeros((len(test), 5))
for fold in range(5):
    model = load_model(f'model_fold{fold}.pth')
    final_pred += model.predict(test) / 5
```

### Strategy 4: Post-Processing
```python
# 1. Negative ê°’ í´ë¦¬í•‘
predictions = np.maximum(predictions, 0)

# 2. íƒ€ê²Ÿ ê°„ ê´€ê³„ í™œìš©
# Dry_Total â‰ˆ Dry_Clover + Dry_Dead + Dry_Green
sum_components = pred_clover + pred_dead + pred_green
pred_total = (pred_total + sum_components) / 2

# 3. GDM ê´€ê³„ í™œìš©
# GDM â‰ˆ Dry_Green + Dry_Clover (ëŒ€ëµì )
pred_gdm = np.clip(pred_gdm, pred_green * 0.8, pred_total)
```

### Strategy 5: Pseudo Labeling (Optional)
```python
# Test ë°ì´í„°ì— ëŒ€í•´ pseudo label ìƒì„± í›„ ì¬í•™ìŠµ
# âš ï¸ ì£¼ì˜: LB probing ìœ„í—˜
```

---

## ğŸ“… 9ì¼ ì‹¤í–‰ ê³„íš

### Day 1-2: ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] DINOv2 Large ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„
- [ ] GroupKFold CV ì„¤ì •
- [ ] ê¸°ë³¸ Augmentation ì ìš©
- [ ] Baseline ì œì¶œ (ëª©í‘œ: 0.65+)

### Day 3-4: ëª¨ë¸ ìµœì í™”
- [ ] Backbone fine-tuning ì‹¤í—˜
- [ ] ë‹¤ì–‘í•œ Head êµ¬ì¡° ì‹¤í—˜
- [ ] Loss function ì‹¤í—˜
- [ ] í•™ìŠµë¥ /ìŠ¤ì¼€ì¤„ëŸ¬ íŠœë‹

### Day 5-6: ì•™ìƒë¸” êµ¬ì¶•
- [ ] ë‹¤ì–‘í•œ seedë¡œ í•™ìŠµ
- [ ] DINOv2 variants ì‹¤í—˜
- [ ] Multi-scale ì‹¤í—˜
- [ ] TTA êµ¬í˜„

### Day 7-8: ìµœì¢… íŠœë‹
- [ ] ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
- [ ] Post-processing ì‹¤í—˜
- [ ] CV-LB correlation ë¶„ì„
- [ ] Final submission ì¤€ë¹„

### Day 9: ë§ˆì§€ë§‰ ì œì¶œ
- [ ] ìµœì¢… ì•™ìƒë¸” ì œì¶œ
- [ ] ì•ˆì „í•œ ë°±ì—… ì œì¶œ

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **CV-LB Gap**: Local CVê°€ ì¢‹ì•„ë„ LBì—ì„œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
   - ë‹¤ì–‘í•œ foldì˜ ê²°ê³¼ë¥¼ í‰ê· 
   - Private LB 47%ë¥¼ ê³ ë ¤í•œ robustí•œ ëª¨ë¸ ì„ íƒ

2. **Overfitting ìœ„í—˜**
   - 357ê°œì˜ ì‘ì€ ì´ë¯¸ì§€ ìˆ˜
   - Backbone freezing ê¶Œì¥
   - Strong augmentation í•„ìˆ˜

3. **íƒ€ê²Ÿ íŠ¹ì„±**
   - Dry_Clover_g: 37.8% zero â†’ ë³„ë„ ì²˜ë¦¬ ê³ ë ¤
   - Dry_Total_g: ê°€ì¤‘ì¹˜ 0.5 â†’ ê°€ì¥ ì§‘ì¤‘!

4. **Research Code Competition**
   - ì½”ë“œ ê³µê°œ ì œí•œ
   - ìì²´ ì†”ë£¨ì…˜ ê°œë°œ í•„ìˆ˜

---

## ğŸ’¡ ì¶”ê°€ ì•„ì´ë””ì–´

1. **ì™¸ë¶€ ë°ì´í„°**: Irish Grass Clover Datasetìœ¼ë¡œ pre-training
2. **Multi-task Learning**: 5ê°œ íƒ€ê²Ÿ ë™ì‹œ í•™ìŠµìœ¼ë¡œ regularization íš¨ê³¼
3. **Auxiliary Loss**: íƒ€ê²Ÿ ê°„ ê´€ê³„ë¥¼ auxiliary lossë¡œ í™œìš©
4. **Attention Visualization**: ëª¨ë¸ì´ ì–´ë””ë¥¼ ë³´ëŠ”ì§€ í™•ì¸í•˜ì—¬ insight íšë“

---

*Strategy Document - Created: 2026-01-19*
*Competition: CSIRO - Image2Biomass Prediction*
*Model: DINOv2 Large (ViT-L/14)*
